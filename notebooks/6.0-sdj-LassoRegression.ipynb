{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.249085100Z",
     "start_time": "2023-06-13T18:27:55.242080400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fboost import outlier_iqr, DataPreparator, FeatureBoosterRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value\n",
    "seed_value= 0\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.306118300Z",
     "start_time": "2023-06-13T18:27:55.248085500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw/alldata.csv')\n",
    "data.columns = ['familia','indice_fam','subposicion_1','subposicion_2','subposicion_3','subposicion_4', 'energia']\n",
    "mask = (data[['subposicion_1','subposicion_2','subposicion_3','subposicion_4']] == 0).sum(axis=1) > 1\n",
    "lista_train = data.loc[mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]\n",
    "lista_test = data.loc[~mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.307115700Z",
     "start_time": "2023-06-13T18:27:55.271000700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "X_train = lista_train.drop(['energia'], axis=1)\n",
    "X_test = lista_test.drop(['energia'], axis=1)\n",
    "\n",
    "# Fit the encoder and transform the data for both train and test dataframes\n",
    "encoded_train = encoder.fit_transform(X_train)\n",
    "encoded_test = encoder.transform(X_test)\n",
    "\n",
    "# Now, 'encoded_train' and 'encoded_test' are numpy arrays, we can convert them back to dataframes:\n",
    "X_train_scaled = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(X_train.columns))\n",
    "X_test_scaled = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(X_test.columns))\n",
    "\n",
    "\n",
    "y_test = lista_test['energia']\n",
    "y_train = lista_train['energia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.323693300Z",
     "start_time": "2023-06-13T18:27:55.282524600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression R2 (Before feature engineering): 0.3361\n",
      "LinearRegression RMSE (Before feature engineering): 3.7005\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#############################################\n",
    "# 4-LINEAR REGRESSION MODEL WITHFEATURE ENGINEERING #\n",
    "#############################################\n",
    "\"\"\"\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "r2_linreg_before = model_lr.score(X_test_scaled, y_test)\n",
    "rmse_linreg_before = np.sqrt(mean_squared_error(y_test, model_lr.predict(X_test_scaled)))\n",
    "print('LinearRegression R2 (Before feature engineering): ' + str(round(r2_linreg_before, 4)))\n",
    "print('LinearRegression RMSE (Before feature engineering): ' + str(round(rmse_linreg_before, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.661514Z",
     "start_time": "2023-06-13T18:27:55.312118Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x275690fc4c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_results = pd.DataFrame(y_train.copy())\n",
    "fit_results.columns = ['Y_TRUE']\n",
    "test_pred_results = pd.DataFrame(y_test.copy())\n",
    "test_pred_results.columns = ['Y_TRUE']\n",
    "\n",
    "fit_results['Y_FIT'] = model_lr.predict(X_train_scaled).ravel()\n",
    "test_pred_results['Y_PRED'] = model_lr.predict(X_test_scaled)\n",
    "\n",
    "fit_results = fit_results.sort_values(by = ['Y_TRUE']).reset_index(drop = True)\n",
    "test_pred_results = test_pred_results.sort_values(by = ['Y_TRUE']).reset_index(drop = True)\n",
    "\n",
    "#### SCATTER PLOT\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(fit_results.index,  fit_results['Y_TRUE'], alpha = 0.4, s = 12, label = 'Y_TRUE')\n",
    "plt.scatter(fit_results.index,  fit_results['Y_FIT'], alpha = 0.4, s = 12, label = 'Y_FIT')\n",
    "plt.xlabel('Y_TRUE_RANK_ASENDING_ORDER (SORTED INDEX)')\n",
    "plt.ylabel('Y')\n",
    "plt.title('LinearRegression - BEFORE Feature Engineering' + '\\n' + 'R2:' + str(round(r2_linreg_before, 4)) + ', RMSE:' + str(round(rmse_linreg_before, 2)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.685126200Z",
     "start_time": "2023-06-13T18:27:55.666377600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# assuming df is your DataFrame\n",
    "df = lista_train.drop(['energia'], axis=1)\n",
    "\n",
    "# replace zeros with np.nan\n",
    "df = df.replace(0, np.nan)\n",
    "\n",
    "# instantiate OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# apply OneHotEncoder to df\n",
    "onehot_df = enc.fit_transform(df)\n",
    "\n",
    "# create H matrix\n",
    "H = np.zeros(df.shape)\n",
    "\n",
    "# specify the position of non-zero values in H\n",
    "for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[1]):\n",
    "        if not np.isnan(df.iloc[i, j]):\n",
    "            H[i, j] = j+1  # or just 1, based on your requirements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.693132Z",
     "start_time": "2023-06-13T18:27:55.684128900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_array = np.concatenate((onehot_df, H), axis=1)\n",
    "X_train_encoded = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))\n",
    "\n",
    "input_shape_train = (X_train_encoded.shape[1],1)\n",
    "# assuming onehot_df_encoded and H are your input data and are numpy arrays\n",
    "# reshape data for Conv1D, the format should be (n_samples, steps, channels)\n",
    "onehot_df_encoded = onehot_df.reshape((170, 24, 1))\n",
    "H_encoded = H.reshape((170, 4, 1))\n",
    "\n",
    "# target data\n",
    "y_train = np.array(y_train)\n",
    "input_shape_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.836830600Z",
     "start_time": "2023-06-13T18:27:55.693132Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assuming df is your DataFrame\n",
    "df_test = lista_test.drop(['energia'], axis=1)\n",
    "\n",
    "# replace zeros with np.nan\n",
    "df_test = df_test.replace(0, np.nan)\n",
    "\n",
    "# instantiate OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# apply OneHotEncoder to df\n",
    "onehot_df_test = enc.fit_transform(df_test)\n",
    "\n",
    "# create H matrix\n",
    "H_test = np.zeros(df_test.shape)\n",
    "\n",
    "# specify the position of non-zero values in H\n",
    "for i in range(df_test.shape[0]):\n",
    "    for j in range(df_test.shape[1]):\n",
    "        if not np.isnan(df_test.iloc[i, j]):\n",
    "            H_test[i, j] = j+1  # or just 1, based on your requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.853970Z",
     "start_time": "2023-06-13T18:27:55.794987200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_df_test_encoded = onehot_df_test.reshape((onehot_df_test.shape[0], 24, 1))\n",
    "H_test_encoded = H_test.reshape((H_test.shape[0], 4, 1))\n",
    "\n",
    "X_test_array = np.concatenate((onehot_df_test, H_test), axis=1)\n",
    "X_test_encoded = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))\n",
    "input_shape_test = (X_test_encoded.shape[1],1)\n",
    "# target data\n",
    "y_test = np.array(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "alpha (Float)\n",
      "{'default': 0.01, 'conditions': [], 'min_value': 0.01, 'max_value': 1.0, 'step': None, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.22151           |0.22151           |alpha\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 61ms/step - loss: 28.3535 - mae: 4.1166 - val_loss: 28.9973 - val_mae: 4.7670\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28.2106 - mae: 4.0764 - val_loss: 27.4583 - val_mae: 4.6084\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 27.3975 - mae: 4.0205 - val_loss: 26.0344 - val_mae: 4.4702\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 26.5758 - mae: 3.9840 - val_loss: 24.7090 - val_mae: 4.3374\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26.5739 - mae: 3.9639 - val_loss: 23.4650 - val_mae: 4.2087\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25.5975 - mae: 3.8974 - val_loss: 22.3042 - val_mae: 4.0843\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25.0670 - mae: 3.8452 - val_loss: 21.2223 - val_mae: 3.9649\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24.6645 - mae: 3.8501 - val_loss: 20.2179 - val_mae: 3.8505\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.3148 - mae: 3.8002 - val_loss: 19.2601 - val_mae: 3.7381\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23.9118 - mae: 3.7985 - val_loss: 18.3686 - val_mae: 3.6429\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23.4412 - mae: 3.7207 - val_loss: 17.5307 - val_mae: 3.5519\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23.0111 - mae: 3.7258 - val_loss: 16.7685 - val_mae: 3.4662\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22.6166 - mae: 3.6690 - val_loss: 16.0395 - val_mae: 3.3821\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22.5216 - mae: 3.6517 - val_loss: 15.3678 - val_mae: 3.3017\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.7768 - mae: 3.5684 - val_loss: 14.7374 - val_mae: 3.2245\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.7009 - mae: 3.5303 - val_loss: 14.1483 - val_mae: 3.1506\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21.1344 - mae: 3.5208 - val_loss: 13.5911 - val_mae: 3.0788\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.7826 - mae: 3.4896 - val_loss: 13.0653 - val_mae: 3.0093\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20.3394 - mae: 3.4882 - val_loss: 12.5809 - val_mae: 2.9434\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.7694 - mae: 3.4063 - val_loss: 12.1106 - val_mae: 2.8776\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.4239 - mae: 3.3691 - val_loss: 11.6746 - val_mae: 2.8151\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.3452 - mae: 3.4105 - val_loss: 11.2830 - val_mae: 2.7569\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.3621 - mae: 3.4046 - val_loss: 10.9017 - val_mae: 2.6993\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18.8716 - mae: 3.3692 - val_loss: 10.5408 - val_mae: 2.6429\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.2719 - mae: 3.2488 - val_loss: 10.1961 - val_mae: 2.5891\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18.4038 - mae: 3.2728 - val_loss: 9.8744 - val_mae: 2.5375\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.9691 - mae: 3.2814 - val_loss: 9.5698 - val_mae: 2.4865\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.9542 - mae: 3.2304 - val_loss: 9.2771 - val_mae: 2.4380\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 17.4405 - mae: 3.1956 - val_loss: 9.0027 - val_mae: 2.3909\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.3022 - mae: 3.1714 - val_loss: 8.7428 - val_mae: 2.3492\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.0775 - mae: 3.1706 - val_loss: 8.5061 - val_mae: 2.3118\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.6001 - mae: 3.1269 - val_loss: 8.2764 - val_mae: 2.2769\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.3678 - mae: 3.1047 - val_loss: 8.0511 - val_mae: 2.2482\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.4013 - mae: 3.0738 - val_loss: 7.8373 - val_mae: 2.2236\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.0853 - mae: 3.0446 - val_loss: 7.6313 - val_mae: 2.1995\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.6964 - mae: 3.0033 - val_loss: 7.4461 - val_mae: 2.1773\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.3887 - mae: 2.9685 - val_loss: 7.2697 - val_mae: 2.1557\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.2212 - mae: 2.9634 - val_loss: 7.0982 - val_mae: 2.1341\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 14.8397 - mae: 2.9121 - val_loss: 6.9369 - val_mae: 2.1134\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.0117 - mae: 2.9596 - val_loss: 6.7779 - val_mae: 2.0926\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.8665 - mae: 2.9307 - val_loss: 6.6260 - val_mae: 2.0724\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.6039 - mae: 2.8970 - val_loss: 6.4820 - val_mae: 2.0530\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.0487 - mae: 2.8438 - val_loss: 6.3508 - val_mae: 2.0349\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.3613 - mae: 2.8705 - val_loss: 6.2233 - val_mae: 2.0170\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.1800 - mae: 2.8384 - val_loss: 6.0953 - val_mae: 1.9987\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.0611 - mae: 2.7872 - val_loss: 5.9762 - val_mae: 1.9814\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.5487 - mae: 2.7947 - val_loss: 5.8571 - val_mae: 1.9639\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.6936 - mae: 2.8347 - val_loss: 5.7426 - val_mae: 1.9466\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.4141 - mae: 2.7794 - val_loss: 5.6357 - val_mae: 1.9302\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.5611 - mae: 2.8036 - val_loss: 5.5347 - val_mae: 1.9144\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.9259 - mae: 2.7457 - val_loss: 5.4318 - val_mae: 1.8981\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.8178 - mae: 2.7035 - val_loss: 5.3347 - val_mae: 1.8823\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.6871 - mae: 2.7290 - val_loss: 5.2429 - val_mae: 1.8666\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.5331 - mae: 2.6819 - val_loss: 5.1507 - val_mae: 1.8510\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.5396 - mae: 2.6774 - val_loss: 5.0667 - val_mae: 1.8365\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.3724 - mae: 2.6408 - val_loss: 4.9853 - val_mae: 1.8247\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.2108 - mae: 2.6211 - val_loss: 4.9043 - val_mae: 1.8140\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.9826 - mae: 2.6214 - val_loss: 4.8288 - val_mae: 1.8037\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.7476 - mae: 2.6113 - val_loss: 4.7567 - val_mae: 1.7937\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.7151 - mae: 2.5973 - val_loss: 4.6856 - val_mae: 1.7837\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.7553 - mae: 2.5934 - val_loss: 4.6164 - val_mae: 1.7738\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.4889 - mae: 2.5878 - val_loss: 4.5552 - val_mae: 1.7648\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3951 - mae: 2.5139 - val_loss: 4.4920 - val_mae: 1.7552\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.1233 - mae: 2.4988 - val_loss: 4.4341 - val_mae: 1.7461\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.1654 - mae: 2.5459 - val_loss: 4.3786 - val_mae: 1.7374\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.2199 - mae: 2.5423 - val_loss: 4.3217 - val_mae: 1.7285\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.8308 - mae: 2.5012 - val_loss: 4.2630 - val_mae: 1.7192\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.8673 - mae: 2.4994 - val_loss: 4.2085 - val_mae: 1.7102\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.8476 - mae: 2.4975 - val_loss: 4.1534 - val_mae: 1.7008\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.8250 - mae: 2.5058 - val_loss: 4.1052 - val_mae: 1.6926\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.5712 - mae: 2.4354 - val_loss: 4.0571 - val_mae: 1.6842\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.5060 - mae: 2.4534 - val_loss: 4.0079 - val_mae: 1.6757\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.5118 - mae: 2.4288 - val_loss: 3.9613 - val_mae: 1.6674\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.5051 - mae: 2.4543 - val_loss: 3.9185 - val_mae: 1.6595\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.3104 - mae: 2.4115 - val_loss: 3.8756 - val_mae: 1.6515\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.1200 - mae: 2.4253 - val_loss: 3.8364 - val_mae: 1.6441\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.1753 - mae: 2.4097 - val_loss: 3.7972 - val_mae: 1.6365\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.1539 - mae: 2.3993 - val_loss: 3.7576 - val_mae: 1.6288\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.0404 - mae: 2.3881 - val_loss: 3.7194 - val_mae: 1.6211\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.8826 - mae: 2.3840 - val_loss: 3.6843 - val_mae: 1.6136\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 38ms/step - loss: 65.1480 - mae: 6.4363 - val_loss: 156.0138 - val_mae: 10.5408\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 65.2591 - mae: 6.5206 - val_loss: 143.8344 - val_mae: 10.1365\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 64.0456 - mae: 6.3872 - val_loss: 133.6557 - val_mae: 9.7795\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 62.9451 - mae: 6.3895 - val_loss: 124.2138 - val_mae: 9.4313\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 61.7984 - mae: 6.2944 - val_loss: 116.1218 - val_mae: 9.1168\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 60.0201 - mae: 6.2422 - val_loss: 108.7759 - val_mae: 8.8177\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 59.2647 - mae: 6.1438 - val_loss: 102.3716 - val_mae: 8.5847\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 57.5299 - mae: 6.0917 - val_loss: 96.5855 - val_mae: 8.3766\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 56.5133 - mae: 6.0968 - val_loss: 91.5014 - val_mae: 8.1851\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 56.7739 - mae: 6.0394 - val_loss: 87.1931 - val_mae: 8.0157\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 55.3703 - mae: 5.9937 - val_loss: 83.0697 - val_mae: 7.8457\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 54.2957 - mae: 5.8888 - val_loss: 79.4325 - val_mae: 7.6894\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 53.7542 - mae: 5.8795 - val_loss: 76.0426 - val_mae: 7.5384\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 52.5397 - mae: 5.8315 - val_loss: 72.9481 - val_mae: 7.3948\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 51.3769 - mae: 5.6890 - val_loss: 70.4012 - val_mae: 7.2731\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 50.1941 - mae: 5.7511 - val_loss: 67.7987 - val_mae: 7.1436\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 49.5475 - mae: 5.6504 - val_loss: 65.3910 - val_mae: 7.0199\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 48.3823 - mae: 5.5246 - val_loss: 63.3034 - val_mae: 6.9095\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 47.6622 - mae: 5.5267 - val_loss: 61.2700 - val_mae: 6.7989\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 46.8588 - mae: 5.5066 - val_loss: 59.6014 - val_mae: 6.7059\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 46.3508 - mae: 5.5327 - val_loss: 57.7842 - val_mae: 6.6020\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 44.8582 - mae: 5.4037 - val_loss: 56.1717 - val_mae: 6.5079\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 44.4310 - mae: 5.3965 - val_loss: 54.5962 - val_mae: 6.4208\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 43.0253 - mae: 5.2841 - val_loss: 53.3465 - val_mae: 6.3505\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 42.8405 - mae: 5.2762 - val_loss: 51.9378 - val_mae: 6.2690\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 42.1925 - mae: 5.2629 - val_loss: 50.7205 - val_mae: 6.1969\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 42.0831 - mae: 5.2868 - val_loss: 49.4910 - val_mae: 6.1228\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 40.9837 - mae: 5.1309 - val_loss: 48.3008 - val_mae: 6.0513\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 40.6799 - mae: 5.0885 - val_loss: 47.1965 - val_mae: 5.9847\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 39.9245 - mae: 5.0325 - val_loss: 46.2829 - val_mae: 5.9280\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 39.5634 - mae: 5.0404 - val_loss: 45.2349 - val_mae: 5.8611\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 38.5776 - mae: 4.9082 - val_loss: 44.3769 - val_mae: 5.8053\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 38.3398 - mae: 4.9991 - val_loss: 43.4326 - val_mae: 5.7428\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 37.9586 - mae: 4.9030 - val_loss: 42.6014 - val_mae: 5.6875\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 36.9853 - mae: 4.8587 - val_loss: 41.7081 - val_mae: 5.6261\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 36.6105 - mae: 4.7702 - val_loss: 40.9694 - val_mae: 5.5739\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35.0163 - mae: 4.8448 - val_loss: 40.3771 - val_mae: 5.5313\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 35.2446 - mae: 4.7450 - val_loss: 39.5924 - val_mae: 5.4745\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 34.6623 - mae: 4.7376 - val_loss: 38.9272 - val_mae: 5.4255\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 33.3664 - mae: 4.6743 - val_loss: 38.2656 - val_mae: 5.3758\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33.3859 - mae: 4.7760 - val_loss: 37.7239 - val_mae: 5.3350\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 33.4944 - mae: 4.6379 - val_loss: 37.0697 - val_mae: 5.2846\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 32.5143 - mae: 4.5629 - val_loss: 36.3936 - val_mae: 5.2324\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 32.5669 - mae: 4.6048 - val_loss: 35.7867 - val_mae: 5.1844\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.6183 - mae: 4.4985 - val_loss: 35.1748 - val_mae: 5.1352\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.4407 - mae: 4.4033 - val_loss: 34.6875 - val_mae: 5.0963\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.4389 - mae: 4.3875 - val_loss: 34.1661 - val_mae: 5.0554\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 30.7786 - mae: 4.4146 - val_loss: 33.6038 - val_mae: 5.0107\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 30.4266 - mae: 4.4317 - val_loss: 33.0535 - val_mae: 4.9666\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.5491 - mae: 4.3599 - val_loss: 32.5627 - val_mae: 4.9270\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 29.7010 - mae: 4.3675 - val_loss: 32.0958 - val_mae: 4.8889\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.0076 - mae: 4.4066 - val_loss: 31.6319 - val_mae: 4.8507\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.4320 - mae: 4.2480 - val_loss: 31.1168 - val_mae: 4.8079\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28.4487 - mae: 4.2534 - val_loss: 30.6142 - val_mae: 4.7658\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 27.3897 - mae: 4.1276 - val_loss: 30.1751 - val_mae: 4.7287\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 27.5971 - mae: 4.1805 - val_loss: 29.7237 - val_mae: 4.6902\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.7820 - mae: 4.1681 - val_loss: 29.3089 - val_mae: 4.6544\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.4151 - mae: 4.1352 - val_loss: 28.9122 - val_mae: 4.6199\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26.4226 - mae: 4.0897 - val_loss: 28.5484 - val_mae: 4.5880\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25.4711 - mae: 4.0687 - val_loss: 28.2492 - val_mae: 4.5616\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.5663 - mae: 4.0607 - val_loss: 27.8509 - val_mae: 4.5263\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25.6885 - mae: 4.0639 - val_loss: 27.4653 - val_mae: 4.4918\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.9313 - mae: 3.9999 - val_loss: 27.1263 - val_mae: 4.4611\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.2168 - mae: 4.0012 - val_loss: 26.7718 - val_mae: 4.4290\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.4987 - mae: 3.9718 - val_loss: 26.4062 - val_mae: 4.3954\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.5257 - mae: 3.9140 - val_loss: 26.0706 - val_mae: 4.3645\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.0328 - mae: 3.8750 - val_loss: 25.7141 - val_mae: 4.3313\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.1770 - mae: 3.8703 - val_loss: 25.3873 - val_mae: 4.3005\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.2704 - mae: 3.8609 - val_loss: 25.0634 - val_mae: 4.2719\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.1606 - mae: 3.8611 - val_loss: 24.7207 - val_mae: 4.2415\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22.9849 - mae: 3.8131 - val_loss: 24.3946 - val_mae: 4.2125\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.0112 - mae: 3.8446 - val_loss: 24.1389 - val_mae: 4.1896\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.0519 - mae: 3.7296 - val_loss: 23.8615 - val_mae: 4.1645\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.2923 - mae: 3.7732 - val_loss: 23.5577 - val_mae: 4.1370\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.7394 - mae: 3.6915 - val_loss: 23.2632 - val_mae: 4.1098\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.8335 - mae: 3.6760 - val_loss: 22.9596 - val_mae: 4.0816\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 21.8813 - mae: 3.7351 - val_loss: 22.7013 - val_mae: 4.0573\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.2902 - mae: 3.6991 - val_loss: 22.4539 - val_mae: 4.0343\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20.9459 - mae: 3.6504 - val_loss: 22.1795 - val_mae: 4.0083\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20.6655 - mae: 3.5993 - val_loss: 21.9233 - val_mae: 3.9840\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 39ms/step - loss: 54.7860 - mae: 5.9970 - val_loss: 76.2244 - val_mae: 7.6457\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 53.5208 - mae: 5.9618 - val_loss: 71.5925 - val_mae: 7.3730\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 52.4744 - mae: 5.8145 - val_loss: 67.3415 - val_mae: 7.1100\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 50.1187 - mae: 5.6318 - val_loss: 63.6672 - val_mae: 6.8695\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 49.9371 - mae: 5.7794 - val_loss: 60.1745 - val_mae: 6.6310\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 49.2466 - mae: 5.7045 - val_loss: 57.0571 - val_mae: 6.4066\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 48.6905 - mae: 5.6656 - val_loss: 54.1803 - val_mae: 6.1894\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 47.3753 - mae: 5.5909 - val_loss: 51.5976 - val_mae: 5.9850\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 46.7295 - mae: 5.4701 - val_loss: 49.2657 - val_mae: 5.7913\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 46.4521 - mae: 5.5295 - val_loss: 47.1368 - val_mae: 5.6067\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 45.2948 - mae: 5.4260 - val_loss: 45.2416 - val_mae: 5.4568\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 44.1878 - mae: 5.3861 - val_loss: 43.5476 - val_mae: 5.3177\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 43.2887 - mae: 5.3140 - val_loss: 41.9583 - val_mae: 5.1826\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 42.3987 - mae: 5.3216 - val_loss: 40.5746 - val_mae: 5.0596\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.4383 - mae: 5.2361 - val_loss: 39.3509 - val_mae: 4.9623\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 40.7614 - mae: 5.1900 - val_loss: 38.1546 - val_mae: 4.8688\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 40.2767 - mae: 5.1470 - val_loss: 37.0382 - val_mae: 4.7909\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 39.7504 - mae: 5.1313 - val_loss: 36.0262 - val_mae: 4.7174\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 38.6081 - mae: 5.0679 - val_loss: 35.0682 - val_mae: 4.6461\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 38.8574 - mae: 5.0907 - val_loss: 34.2278 - val_mae: 4.5945\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 37.9991 - mae: 4.9522 - val_loss: 33.3874 - val_mae: 4.5409\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 36.7891 - mae: 4.9913 - val_loss: 32.6323 - val_mae: 4.4910\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35.6487 - mae: 4.8240 - val_loss: 31.9214 - val_mae: 4.4455\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35.9416 - mae: 4.8479 - val_loss: 31.2535 - val_mae: 4.4080\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 34.5872 - mae: 4.8584 - val_loss: 30.6610 - val_mae: 4.3735\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.9006 - mae: 4.7667 - val_loss: 30.0746 - val_mae: 4.3380\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 33.6750 - mae: 4.7672 - val_loss: 29.5364 - val_mae: 4.3045\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 33.9489 - mae: 4.7121 - val_loss: 29.0062 - val_mae: 4.2702\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 32.9982 - mae: 4.6862 - val_loss: 28.4924 - val_mae: 4.2358\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33.0026 - mae: 4.6802 - val_loss: 28.0534 - val_mae: 4.2058\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 32.0756 - mae: 4.5074 - val_loss: 27.5888 - val_mae: 4.1733\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.7477 - mae: 4.4938 - val_loss: 27.1752 - val_mae: 4.1484\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 30.3014 - mae: 4.5524 - val_loss: 26.8526 - val_mae: 4.1295\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.2303 - mae: 4.4716 - val_loss: 26.4488 - val_mae: 4.1046\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.8772 - mae: 4.4182 - val_loss: 26.0513 - val_mae: 4.0797\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 29.4873 - mae: 4.4249 - val_loss: 25.6588 - val_mae: 4.0543\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 28.9719 - mae: 4.3688 - val_loss: 25.3107 - val_mae: 4.0315\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.6293 - mae: 4.3667 - val_loss: 24.9651 - val_mae: 4.0078\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.4940 - mae: 4.3478 - val_loss: 24.6298 - val_mae: 3.9842\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.3721 - mae: 4.3166 - val_loss: 24.3225 - val_mae: 3.9621\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.2840 - mae: 4.1896 - val_loss: 24.0718 - val_mae: 3.9439\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26.8674 - mae: 4.2340 - val_loss: 23.7587 - val_mae: 3.9210\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26.1353 - mae: 4.1651 - val_loss: 23.4725 - val_mae: 3.8995\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.1494 - mae: 4.1556 - val_loss: 23.1901 - val_mae: 3.8779\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.2319 - mae: 4.0635 - val_loss: 22.8847 - val_mae: 3.8541\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.9250 - mae: 4.0956 - val_loss: 22.6015 - val_mae: 3.8321\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.1213 - mae: 4.1057 - val_loss: 22.3015 - val_mae: 3.8084\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 24.6865 - mae: 3.9709 - val_loss: 22.0473 - val_mae: 3.7877\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.7208 - mae: 4.0811 - val_loss: 21.7983 - val_mae: 3.7672\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 24.1788 - mae: 3.9595 - val_loss: 21.5360 - val_mae: 3.7456\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.6594 - mae: 3.9787 - val_loss: 21.2649 - val_mae: 3.7230\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.4354 - mae: 3.9003 - val_loss: 21.0164 - val_mae: 3.7019\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.9871 - mae: 3.9256 - val_loss: 20.7777 - val_mae: 3.6813\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22.7330 - mae: 3.8822 - val_loss: 20.5235 - val_mae: 3.6595\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.3483 - mae: 3.9103 - val_loss: 20.2880 - val_mae: 3.6391\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.3102 - mae: 3.8427 - val_loss: 20.0530 - val_mae: 3.6183\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.8779 - mae: 3.7805 - val_loss: 19.8429 - val_mae: 3.5994\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 21.4503 - mae: 3.7963 - val_loss: 19.6199 - val_mae: 3.5796\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 21.2615 - mae: 3.7537 - val_loss: 19.4165 - val_mae: 3.5611\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.7238 - mae: 3.7440 - val_loss: 19.1817 - val_mae: 3.5397\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.2373 - mae: 3.6828 - val_loss: 18.9928 - val_mae: 3.5221\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.4557 - mae: 3.7396 - val_loss: 18.7985 - val_mae: 3.5041\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 20.5066 - mae: 3.7050 - val_loss: 18.6129 - val_mae: 3.4866\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 20.0028 - mae: 3.6731 - val_loss: 18.3921 - val_mae: 3.4658\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.8610 - mae: 3.6433 - val_loss: 18.2143 - val_mae: 3.4487\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.1537 - mae: 3.4901 - val_loss: 18.0683 - val_mae: 3.4346\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.4461 - mae: 3.5861 - val_loss: 17.8693 - val_mae: 3.4156\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.1659 - mae: 3.5156 - val_loss: 17.7154 - val_mae: 3.4005\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.1507 - mae: 3.4534 - val_loss: 17.5425 - val_mae: 3.3837\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.8621 - mae: 3.4941 - val_loss: 17.3482 - val_mae: 3.3648\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.1618 - mae: 3.4683 - val_loss: 17.1557 - val_mae: 3.3458\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.2781 - mae: 3.4642 - val_loss: 16.9648 - val_mae: 3.3267\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.1142 - mae: 3.4689 - val_loss: 16.7832 - val_mae: 3.3086\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 17.5244 - mae: 3.4162 - val_loss: 16.5950 - val_mae: 3.2895\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.7715 - mae: 3.4080 - val_loss: 16.4270 - val_mae: 3.2724\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.3484 - mae: 3.3816 - val_loss: 16.2529 - val_mae: 3.2544\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.3498 - mae: 3.3686 - val_loss: 16.0676 - val_mae: 3.2350\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.9244 - mae: 3.3084 - val_loss: 15.9100 - val_mae: 3.2183\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.1443 - mae: 3.3541 - val_loss: 15.7455 - val_mae: 3.2010\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.1822 - mae: 3.3396 - val_loss: 15.5644 - val_mae: 3.1818\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 55.8986 - mae: 6.3214 - val_loss: 72.1617 - val_mae: 6.8881\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 54.9101 - mae: 6.3782 - val_loss: 66.9364 - val_mae: 6.6316\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 54.4697 - mae: 6.2941 - val_loss: 62.3431 - val_mae: 6.3968\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 53.1708 - mae: 6.2292 - val_loss: 58.3480 - val_mae: 6.1849\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 52.3069 - mae: 6.2223 - val_loss: 54.6798 - val_mae: 5.9829\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 51.2319 - mae: 6.1823 - val_loss: 51.4787 - val_mae: 5.8005\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 50.5354 - mae: 6.1111 - val_loss: 48.6210 - val_mae: 5.6319\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 50.0206 - mae: 6.0552 - val_loss: 46.0334 - val_mae: 5.4744\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 48.2652 - mae: 5.9049 - val_loss: 43.7363 - val_mae: 5.3301\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 47.6741 - mae: 5.9482 - val_loss: 41.6917 - val_mae: 5.1977\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 46.7501 - mae: 5.8853 - val_loss: 39.7427 - val_mae: 5.0679\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 45.9232 - mae: 5.8280 - val_loss: 37.9377 - val_mae: 4.9440\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 44.2366 - mae: 5.7529 - val_loss: 36.3656 - val_mae: 4.8379\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 43.5828 - mae: 5.6355 - val_loss: 34.8218 - val_mae: 4.7379\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 43.4314 - mae: 5.6000 - val_loss: 33.4640 - val_mae: 4.6474\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 42.6547 - mae: 5.5834 - val_loss: 32.0959 - val_mae: 4.5536\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 42.4444 - mae: 5.5469 - val_loss: 30.8717 - val_mae: 4.4674\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 41.0968 - mae: 5.3269 - val_loss: 29.8016 - val_mae: 4.3911\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 40.2790 - mae: 5.3619 - val_loss: 28.7275 - val_mae: 4.3119\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 39.9093 - mae: 5.3536 - val_loss: 27.7228 - val_mae: 4.2389\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 39.1316 - mae: 5.3727 - val_loss: 26.8141 - val_mae: 4.1751\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 38.4083 - mae: 5.2966 - val_loss: 25.9142 - val_mae: 4.1104\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 37.7824 - mae: 5.2590 - val_loss: 25.0962 - val_mae: 4.0500\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 37.0850 - mae: 5.1632 - val_loss: 24.2914 - val_mae: 3.9890\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 35.3238 - mae: 5.0067 - val_loss: 23.5765 - val_mae: 3.9351\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 36.2340 - mae: 5.1335 - val_loss: 22.8546 - val_mae: 3.8858\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 35.6843 - mae: 5.0244 - val_loss: 22.2160 - val_mae: 3.8423\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 34.4595 - mae: 5.0068 - val_loss: 21.6302 - val_mae: 3.8014\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.1863 - mae: 5.0046 - val_loss: 21.0258 - val_mae: 3.7584\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 33.9106 - mae: 4.9522 - val_loss: 20.4464 - val_mae: 3.7156\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.5178 - mae: 4.7990 - val_loss: 19.9073 - val_mae: 3.6749\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.6359 - mae: 4.8430 - val_loss: 19.3964 - val_mae: 3.6368\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.3114 - mae: 4.8204 - val_loss: 18.9342 - val_mae: 3.6009\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.6937 - mae: 4.7986 - val_loss: 18.4651 - val_mae: 3.5635\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 31.1765 - mae: 4.7174 - val_loss: 18.0252 - val_mae: 3.5279\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 30.6933 - mae: 4.7194 - val_loss: 17.6057 - val_mae: 3.4933\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.0890 - mae: 4.6435 - val_loss: 17.1846 - val_mae: 3.4577\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.8706 - mae: 4.5982 - val_loss: 16.7933 - val_mae: 3.4242\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 29.0075 - mae: 4.5880 - val_loss: 16.4379 - val_mae: 3.3932\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28.6202 - mae: 4.5388 - val_loss: 16.1117 - val_mae: 3.3639\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.6549 - mae: 4.5368 - val_loss: 15.7740 - val_mae: 3.3336\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.5637 - mae: 4.4533 - val_loss: 15.4817 - val_mae: 3.3068\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 27.5676 - mae: 4.4258 - val_loss: 15.1555 - val_mae: 3.2761\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.0374 - mae: 4.3424 - val_loss: 14.8904 - val_mae: 3.2509\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.9714 - mae: 4.3581 - val_loss: 14.5928 - val_mae: 3.2222\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.7721 - mae: 4.4104 - val_loss: 14.3446 - val_mae: 3.1980\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.1000 - mae: 4.2218 - val_loss: 14.0617 - val_mae: 3.1693\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25.5849 - mae: 4.2595 - val_loss: 13.7922 - val_mae: 3.1418\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25.5701 - mae: 4.2583 - val_loss: 13.5432 - val_mae: 3.1164\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.1165 - mae: 4.1639 - val_loss: 13.3155 - val_mae: 3.0921\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.9580 - mae: 4.1751 - val_loss: 13.1023 - val_mae: 3.0690\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.6438 - mae: 4.1585 - val_loss: 12.8681 - val_mae: 3.0434\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24.2477 - mae: 4.0575 - val_loss: 12.6597 - val_mae: 3.0220\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 23.7167 - mae: 4.0475 - val_loss: 12.4543 - val_mae: 3.0008\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.5755 - mae: 4.0338 - val_loss: 12.2348 - val_mae: 2.9781\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.4222 - mae: 4.0052 - val_loss: 12.0274 - val_mae: 2.9561\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.0384 - mae: 3.9902 - val_loss: 11.8301 - val_mae: 2.9347\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.7472 - mae: 3.9571 - val_loss: 11.6349 - val_mae: 2.9138\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.7863 - mae: 3.8761 - val_loss: 11.4452 - val_mae: 2.8923\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.7986 - mae: 3.8536 - val_loss: 11.2804 - val_mae: 2.8733\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.6308 - mae: 3.9008 - val_loss: 11.1009 - val_mae: 2.8527\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.3697 - mae: 3.8558 - val_loss: 10.9339 - val_mae: 2.8331\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20.8566 - mae: 3.8341 - val_loss: 10.7757 - val_mae: 2.8143\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.5763 - mae: 3.8346 - val_loss: 10.6267 - val_mae: 2.7970\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 20.5953 - mae: 3.7538 - val_loss: 10.4762 - val_mae: 2.7787\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 20.3695 - mae: 3.7406 - val_loss: 10.3313 - val_mae: 2.7611\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.9052 - mae: 3.7006 - val_loss: 10.1890 - val_mae: 2.7432\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19.8687 - mae: 3.6935 - val_loss: 10.0536 - val_mae: 2.7260\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 19.7536 - mae: 3.6631 - val_loss: 9.9175 - val_mae: 2.7080\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.1707 - mae: 3.5856 - val_loss: 9.7874 - val_mae: 2.6907\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.9352 - mae: 3.5818 - val_loss: 9.6584 - val_mae: 2.6733\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.8617 - mae: 3.5789 - val_loss: 9.5389 - val_mae: 2.6572\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.0770 - mae: 3.5874 - val_loss: 9.4153 - val_mae: 2.6402\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.1063 - mae: 3.5569 - val_loss: 9.3077 - val_mae: 2.6253\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.5382 - mae: 3.5272 - val_loss: 9.1937 - val_mae: 2.6090\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.2900 - mae: 3.4896 - val_loss: 9.0924 - val_mae: 2.5946\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.8779 - mae: 3.4388 - val_loss: 8.9868 - val_mae: 2.5793\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.8959 - mae: 3.4817 - val_loss: 8.8742 - val_mae: 2.5626\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.3903 - mae: 3.4447 - val_loss: 8.7775 - val_mae: 2.5481\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.1813 - mae: 3.3665 - val_loss: 8.6826 - val_mae: 2.5339\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 38ms/step - loss: 49.2789 - mae: 5.5995 - val_loss: 52.4398 - val_mae: 6.2520\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 48.3029 - mae: 5.4745 - val_loss: 49.8325 - val_mae: 6.0519\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 47.4830 - mae: 5.3212 - val_loss: 47.4389 - val_mae: 5.8558\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 46.0546 - mae: 5.2802 - val_loss: 45.2933 - val_mae: 5.6676\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 45.9517 - mae: 5.3055 - val_loss: 43.3955 - val_mae: 5.4891\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 44.8199 - mae: 5.2626 - val_loss: 41.6515 - val_mae: 5.3157\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 44.1550 - mae: 5.1880 - val_loss: 40.1636 - val_mae: 5.1555\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 42.6868 - mae: 5.1240 - val_loss: 38.7987 - val_mae: 5.0000\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 41.7843 - mae: 5.0905 - val_loss: 37.5790 - val_mae: 4.8534\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 40.3169 - mae: 4.9711 - val_loss: 36.4906 - val_mae: 4.7355\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.0002 - mae: 4.9972 - val_loss: 35.5537 - val_mae: 4.6280\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 40.5282 - mae: 4.9130 - val_loss: 34.8087 - val_mae: 4.5327\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 39.2713 - mae: 4.7713 - val_loss: 34.0449 - val_mae: 4.4373\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 38.1387 - mae: 4.8193 - val_loss: 33.3405 - val_mae: 4.3656\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 37.9378 - mae: 4.8193 - val_loss: 32.7257 - val_mae: 4.3231\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 37.2971 - mae: 4.8042 - val_loss: 32.1442 - val_mae: 4.2822\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 36.3893 - mae: 4.7357 - val_loss: 31.6111 - val_mae: 4.2425\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 36.3501 - mae: 4.6834 - val_loss: 31.1789 - val_mae: 4.2080\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 34.8603 - mae: 4.5941 - val_loss: 30.7149 - val_mae: 4.1814\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 34.2325 - mae: 4.5339 - val_loss: 30.2855 - val_mae: 4.1568\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.3703 - mae: 4.6079 - val_loss: 29.9013 - val_mae: 4.1379\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 32.9758 - mae: 4.4382 - val_loss: 29.4990 - val_mae: 4.1224\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.7993 - mae: 4.4865 - val_loss: 29.1106 - val_mae: 4.1058\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.0371 - mae: 4.4261 - val_loss: 28.7960 - val_mae: 4.0923\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.4098 - mae: 4.3812 - val_loss: 28.4819 - val_mae: 4.0775\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.4080 - mae: 4.3607 - val_loss: 28.1752 - val_mae: 4.0623\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.3515 - mae: 4.3357 - val_loss: 27.8685 - val_mae: 4.0456\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.7813 - mae: 4.2952 - val_loss: 27.5552 - val_mae: 4.0274\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.7573 - mae: 4.2221 - val_loss: 27.2637 - val_mae: 4.0100\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.9292 - mae: 4.2524 - val_loss: 26.9892 - val_mae: 3.9991\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 27.8858 - mae: 4.1668 - val_loss: 26.7214 - val_mae: 3.9889\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 27.8942 - mae: 4.1978 - val_loss: 26.4524 - val_mae: 3.9771\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.0762 - mae: 3.9726 - val_loss: 26.1614 - val_mae: 3.9634\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.6802 - mae: 4.0677 - val_loss: 25.8656 - val_mae: 3.9482\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26.4847 - mae: 4.0493 - val_loss: 25.5787 - val_mae: 3.9331\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.1238 - mae: 4.0063 - val_loss: 25.3090 - val_mae: 3.9179\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.3626 - mae: 3.9872 - val_loss: 25.0354 - val_mae: 3.9019\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.0947 - mae: 4.0046 - val_loss: 24.7832 - val_mae: 3.8870\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.7905 - mae: 3.9128 - val_loss: 24.5068 - val_mae: 3.8702\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.3058 - mae: 3.8394 - val_loss: 24.2242 - val_mae: 3.8519\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.5936 - mae: 3.8404 - val_loss: 23.9876 - val_mae: 3.8365\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.7343 - mae: 3.7797 - val_loss: 23.7161 - val_mae: 3.8189\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.6779 - mae: 3.8166 - val_loss: 23.4068 - val_mae: 3.7976\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.9481 - mae: 3.6830 - val_loss: 23.1799 - val_mae: 3.7818\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.0215 - mae: 3.7966 - val_loss: 22.9499 - val_mae: 3.7653\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.4330 - mae: 3.7158 - val_loss: 22.6712 - val_mae: 3.7456\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.9643 - mae: 3.6608 - val_loss: 22.4091 - val_mae: 3.7266\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.7046 - mae: 3.6042 - val_loss: 22.1486 - val_mae: 3.7072\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 22.0096 - mae: 3.6668 - val_loss: 21.9018 - val_mae: 3.6883\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20.6579 - mae: 3.6075 - val_loss: 21.6353 - val_mae: 3.6678\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.2448 - mae: 3.5129 - val_loss: 21.3704 - val_mae: 3.6476\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.3447 - mae: 3.5423 - val_loss: 21.1204 - val_mae: 3.6280\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.6471 - mae: 3.4185 - val_loss: 20.8800 - val_mae: 3.6091\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.3775 - mae: 3.3890 - val_loss: 20.6404 - val_mae: 3.5899\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.3666 - mae: 3.4523 - val_loss: 20.4388 - val_mae: 3.5737\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.3886 - mae: 3.4783 - val_loss: 20.2315 - val_mae: 3.5570\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.9217 - mae: 3.4141 - val_loss: 19.9599 - val_mae: 3.5346\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.0852 - mae: 3.4291 - val_loss: 19.7020 - val_mae: 3.5133\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.6300 - mae: 3.3953 - val_loss: 19.4706 - val_mae: 3.4937\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.6896 - mae: 3.3565 - val_loss: 19.2377 - val_mae: 3.4737\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.0257 - mae: 3.3322 - val_loss: 18.9849 - val_mae: 3.4522\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.5106 - mae: 3.3332 - val_loss: 18.7576 - val_mae: 3.4323\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.4971 - mae: 3.2808 - val_loss: 18.5213 - val_mae: 3.4114\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.2092 - mae: 3.2539 - val_loss: 18.2984 - val_mae: 3.3918\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.0475 - mae: 3.2007 - val_loss: 18.0832 - val_mae: 3.3726\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.3974 - mae: 3.2802 - val_loss: 17.9190 - val_mae: 3.3575\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.9638 - mae: 3.1945 - val_loss: 17.7061 - val_mae: 3.3379\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.7459 - mae: 3.2057 - val_loss: 17.4826 - val_mae: 3.3173\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.0282 - mae: 3.2411 - val_loss: 17.2906 - val_mae: 3.2993\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.6233 - mae: 3.1951 - val_loss: 17.0851 - val_mae: 3.2798\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 16.1124 - mae: 3.1729 - val_loss: 16.9005 - val_mae: 3.2621\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.0761 - mae: 3.1576 - val_loss: 16.7080 - val_mae: 3.2438\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.9212 - mae: 3.1309 - val_loss: 16.5182 - val_mae: 3.2252\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.6254 - mae: 3.0613 - val_loss: 16.3205 - val_mae: 3.2058\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.5794 - mae: 3.0651 - val_loss: 16.1163 - val_mae: 3.1858\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.2108 - mae: 3.0477 - val_loss: 15.9168 - val_mae: 3.1661\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.1334 - mae: 3.0537 - val_loss: 15.7281 - val_mae: 3.1473\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.0196 - mae: 2.9983 - val_loss: 15.5469 - val_mae: 3.1289\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.7185 - mae: 2.9972 - val_loss: 15.3502 - val_mae: 3.1089\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.3333 - mae: 2.9442 - val_loss: 15.1781 - val_mae: 3.0909\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 36ms/step - loss: 64.8054 - mae: 6.1378 - val_loss: 88.1008 - val_mae: 7.6011\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 64.8348 - mae: 6.1354 - val_loss: 85.4970 - val_mae: 7.4659\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 62.6235 - mae: 5.9390 - val_loss: 82.9119 - val_mae: 7.3312\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 61.2395 - mae: 5.9332 - val_loss: 80.3650 - val_mae: 7.1961\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 60.1918 - mae: 6.0333 - val_loss: 78.1708 - val_mae: 7.0765\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 59.1460 - mae: 5.7965 - val_loss: 75.9021 - val_mae: 6.9519\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 58.6387 - mae: 5.8015 - val_loss: 73.8158 - val_mae: 6.8347\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 57.5279 - mae: 5.7293 - val_loss: 71.7925 - val_mae: 6.7178\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 55.1850 - mae: 5.6187 - val_loss: 69.9993 - val_mae: 6.6124\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 54.7259 - mae: 5.5805 - val_loss: 68.1326 - val_mae: 6.5026\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 55.1325 - mae: 5.6541 - val_loss: 66.4089 - val_mae: 6.3968\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 53.4732 - mae: 5.5460 - val_loss: 64.7835 - val_mae: 6.2980\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 52.2237 - mae: 5.4598 - val_loss: 63.1802 - val_mae: 6.1978\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 50.8056 - mae: 5.4192 - val_loss: 61.6197 - val_mae: 6.1027\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 50.1729 - mae: 5.4778 - val_loss: 60.2371 - val_mae: 6.0215\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 49.2887 - mae: 5.3297 - val_loss: 58.8566 - val_mae: 5.9405\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 49.4882 - mae: 5.2594 - val_loss: 57.4887 - val_mae: 5.8600\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 47.4994 - mae: 5.0690 - val_loss: 56.1959 - val_mae: 5.7823\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 46.1436 - mae: 5.1604 - val_loss: 54.9954 - val_mae: 5.7083\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 46.7491 - mae: 5.1323 - val_loss: 53.7897 - val_mae: 5.6328\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 45.2756 - mae: 5.0946 - val_loss: 52.6006 - val_mae: 5.5589\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 44.8620 - mae: 5.0695 - val_loss: 51.5992 - val_mae: 5.4948\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 43.2362 - mae: 4.9911 - val_loss: 50.6190 - val_mae: 5.4316\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 43.2311 - mae: 4.9218 - val_loss: 49.6059 - val_mae: 5.3656\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 41.1089 - mae: 4.8884 - val_loss: 48.7134 - val_mae: 5.3056\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 41.5377 - mae: 4.8760 - val_loss: 47.7238 - val_mae: 5.2390\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.1635 - mae: 4.8955 - val_loss: 46.8182 - val_mae: 5.1770\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 39.8052 - mae: 4.7976 - val_loss: 45.8856 - val_mae: 5.1136\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 39.8201 - mae: 4.7932 - val_loss: 45.0300 - val_mae: 5.0548\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 39.0952 - mae: 4.7397 - val_loss: 44.2066 - val_mae: 5.0036\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 37.8048 - mae: 4.6415 - val_loss: 43.4108 - val_mae: 4.9539\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 38.4849 - mae: 4.7878 - val_loss: 42.7182 - val_mae: 4.9093\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 36.6721 - mae: 4.6265 - val_loss: 41.9606 - val_mae: 4.8669\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 36.1125 - mae: 4.6517 - val_loss: 41.2453 - val_mae: 4.8269\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 35.8949 - mae: 4.4937 - val_loss: 40.5790 - val_mae: 4.7892\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.7849 - mae: 4.6722 - val_loss: 39.9887 - val_mae: 4.7552\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 34.1047 - mae: 4.5255 - val_loss: 39.3257 - val_mae: 4.7169\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.4286 - mae: 4.5338 - val_loss: 38.6523 - val_mae: 4.6774\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33.6450 - mae: 4.4973 - val_loss: 38.0230 - val_mae: 4.6403\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.8659 - mae: 4.3149 - val_loss: 37.4172 - val_mae: 4.6040\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.6819 - mae: 4.3841 - val_loss: 36.7852 - val_mae: 4.5658\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.1606 - mae: 4.3833 - val_loss: 36.2748 - val_mae: 4.5346\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.9493 - mae: 4.4390 - val_loss: 35.7620 - val_mae: 4.5027\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.2111 - mae: 4.3021 - val_loss: 35.1620 - val_mae: 4.4653\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 31.1403 - mae: 4.2365 - val_loss: 34.6360 - val_mae: 4.4316\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.3401 - mae: 4.2678 - val_loss: 34.0777 - val_mae: 4.3960\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.5765 - mae: 4.1887 - val_loss: 33.5373 - val_mae: 4.3613\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 29.2920 - mae: 4.1802 - val_loss: 32.9875 - val_mae: 4.3257\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 29.1503 - mae: 4.1333 - val_loss: 32.4846 - val_mae: 4.2925\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.7097 - mae: 4.1501 - val_loss: 31.9624 - val_mae: 4.2580\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.5564 - mae: 4.0626 - val_loss: 31.4839 - val_mae: 4.2260\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.7276 - mae: 4.0108 - val_loss: 30.9984 - val_mae: 4.1929\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.9469 - mae: 4.1351 - val_loss: 30.5297 - val_mae: 4.1606\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.9552 - mae: 3.9970 - val_loss: 30.0514 - val_mae: 4.1274\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.8745 - mae: 3.9753 - val_loss: 29.6260 - val_mae: 4.0975\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.2290 - mae: 3.9518 - val_loss: 29.1788 - val_mae: 4.0659\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25.7575 - mae: 3.9526 - val_loss: 28.7633 - val_mae: 4.0361\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.7485 - mae: 3.9331 - val_loss: 28.3725 - val_mae: 4.0077\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.0079 - mae: 3.8875 - val_loss: 27.9813 - val_mae: 3.9792\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 25.3930 - mae: 3.9383 - val_loss: 27.6301 - val_mae: 3.9536\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 24.7716 - mae: 3.9535 - val_loss: 27.2697 - val_mae: 3.9267\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24.5354 - mae: 3.8278 - val_loss: 26.8690 - val_mae: 3.8968\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.2520 - mae: 3.7882 - val_loss: 26.5330 - val_mae: 3.8714\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.2207 - mae: 3.8067 - val_loss: 26.2176 - val_mae: 3.8465\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.7761 - mae: 3.8141 - val_loss: 25.8681 - val_mae: 3.8199\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.6075 - mae: 3.7205 - val_loss: 25.5325 - val_mae: 3.7939\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 22.9917 - mae: 3.7329 - val_loss: 25.1932 - val_mae: 3.7678\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.0212 - mae: 3.6566 - val_loss: 24.9015 - val_mae: 3.7455\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.2388 - mae: 3.7061 - val_loss: 24.6044 - val_mae: 3.7227\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.5264 - mae: 3.7178 - val_loss: 24.2567 - val_mae: 3.6957\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.6954 - mae: 3.6597 - val_loss: 23.9861 - val_mae: 3.6745\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.5659 - mae: 3.7307 - val_loss: 23.7712 - val_mae: 3.6573\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.9633 - mae: 3.6567 - val_loss: 23.5100 - val_mae: 3.6365\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 21.0889 - mae: 3.5357 - val_loss: 23.2500 - val_mae: 3.6156\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21.4241 - mae: 3.6587 - val_loss: 22.9797 - val_mae: 3.5937\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.8588 - mae: 3.6374 - val_loss: 22.6866 - val_mae: 3.5699\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.7315 - mae: 3.5720 - val_loss: 22.3857 - val_mae: 3.5452\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.3828 - mae: 3.5160 - val_loss: 22.1097 - val_mae: 3.5222\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.7460 - mae: 3.4809 - val_loss: 21.8503 - val_mae: 3.5005\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.0426 - mae: 3.5573 - val_loss: 21.6026 - val_mae: 3.4796\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 40ms/step - loss: 41.9199 - mae: 5.3205 - val_loss: 101.3148 - val_mae: 8.4423\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.0426 - mae: 5.2422 - val_loss: 93.6091 - val_mae: 8.0861\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 40.6165 - mae: 5.2039 - val_loss: 86.7640 - val_mae: 7.7969\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 40.3032 - mae: 5.2012 - val_loss: 80.6505 - val_mae: 7.5272\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 38.8444 - mae: 5.1088 - val_loss: 75.2676 - val_mae: 7.2795\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 38.3262 - mae: 5.1114 - val_loss: 70.4437 - val_mae: 7.0486\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 37.9077 - mae: 5.0740 - val_loss: 66.0970 - val_mae: 6.8324\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 37.1218 - mae: 4.9315 - val_loss: 62.0601 - val_mae: 6.6242\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 36.3177 - mae: 4.9389 - val_loss: 58.3654 - val_mae: 6.4264\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 35.6736 - mae: 4.8428 - val_loss: 55.0276 - val_mae: 6.2414\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 34.9390 - mae: 4.8071 - val_loss: 51.9412 - val_mae: 6.0642\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.4449 - mae: 4.8428 - val_loss: 49.2725 - val_mae: 5.9053\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33.8263 - mae: 4.7423 - val_loss: 46.7035 - val_mae: 5.7479\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33.2057 - mae: 4.6914 - val_loss: 44.3501 - val_mae: 5.5989\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 32.8608 - mae: 4.6680 - val_loss: 42.1543 - val_mae: 5.4554\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31.5540 - mae: 4.5441 - val_loss: 40.2263 - val_mae: 5.3254\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.1574 - mae: 4.5521 - val_loss: 38.3588 - val_mae: 5.2101\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 30.7198 - mae: 4.5264 - val_loss: 36.6861 - val_mae: 5.1033\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 30.1534 - mae: 4.4930 - val_loss: 35.1222 - val_mae: 5.0009\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 29.6494 - mae: 4.4460 - val_loss: 33.6722 - val_mae: 4.9031\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 29.5619 - mae: 4.4174 - val_loss: 32.2736 - val_mae: 4.8064\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.7371 - mae: 4.3545 - val_loss: 31.0031 - val_mae: 4.7160\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28.6470 - mae: 4.3613 - val_loss: 29.8155 - val_mae: 4.6292\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 27.2815 - mae: 4.2401 - val_loss: 28.7491 - val_mae: 4.5492\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 27.0747 - mae: 4.2785 - val_loss: 27.6984 - val_mae: 4.4686\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.4106 - mae: 4.2157 - val_loss: 26.7371 - val_mae: 4.3927\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.4905 - mae: 4.1579 - val_loss: 25.8596 - val_mae: 4.3219\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26.0924 - mae: 4.1868 - val_loss: 24.9982 - val_mae: 4.2504\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.3006 - mae: 4.1292 - val_loss: 24.1858 - val_mae: 4.1823\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.8413 - mae: 4.0611 - val_loss: 23.4021 - val_mae: 4.1152\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 24.7664 - mae: 4.0544 - val_loss: 22.6595 - val_mae: 4.0501\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.9953 - mae: 4.0038 - val_loss: 21.9744 - val_mae: 3.9887\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.2388 - mae: 4.0175 - val_loss: 21.3597 - val_mae: 3.9319\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.5220 - mae: 3.9381 - val_loss: 20.7493 - val_mae: 3.8749\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.3353 - mae: 3.9486 - val_loss: 20.1655 - val_mae: 3.8193\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22.8529 - mae: 3.9269 - val_loss: 19.6160 - val_mae: 3.7659\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.5761 - mae: 3.8195 - val_loss: 19.0895 - val_mae: 3.7141\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 22.1344 - mae: 3.8269 - val_loss: 18.5911 - val_mae: 3.6636\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 22.3381 - mae: 3.8174 - val_loss: 18.1165 - val_mae: 3.6145\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21.0669 - mae: 3.7530 - val_loss: 17.6563 - val_mae: 3.5667\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20.8636 - mae: 3.6858 - val_loss: 17.2562 - val_mae: 3.5239\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.3827 - mae: 3.6498 - val_loss: 16.8471 - val_mae: 3.4794\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.2293 - mae: 3.6869 - val_loss: 16.4483 - val_mae: 3.4359\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20.5629 - mae: 3.6809 - val_loss: 16.0680 - val_mae: 3.3935\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20.2344 - mae: 3.6083 - val_loss: 15.7080 - val_mae: 3.3529\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.1763 - mae: 3.6827 - val_loss: 15.3470 - val_mae: 3.3112\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.6507 - mae: 3.5627 - val_loss: 15.0023 - val_mae: 3.2710\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.0472 - mae: 3.5526 - val_loss: 14.6828 - val_mae: 3.2333\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.9815 - mae: 3.5231 - val_loss: 14.3681 - val_mae: 3.2011\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 18.6596 - mae: 3.4823 - val_loss: 14.0697 - val_mae: 3.1705\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 18.6294 - mae: 3.4948 - val_loss: 13.7670 - val_mae: 3.1390\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.2937 - mae: 3.4731 - val_loss: 13.4965 - val_mae: 3.1101\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.2684 - mae: 3.4773 - val_loss: 13.2415 - val_mae: 3.0822\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.8316 - mae: 3.4065 - val_loss: 12.9877 - val_mae: 3.0545\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.2688 - mae: 3.3452 - val_loss: 12.7566 - val_mae: 3.0290\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.2473 - mae: 3.3875 - val_loss: 12.5208 - val_mae: 3.0027\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.0926 - mae: 3.3431 - val_loss: 12.3002 - val_mae: 2.9774\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.7650 - mae: 3.3360 - val_loss: 12.0849 - val_mae: 2.9525\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16.7394 - mae: 3.3013 - val_loss: 11.8603 - val_mae: 2.9263\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 16.6347 - mae: 3.2831 - val_loss: 11.6667 - val_mae: 2.9035\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.4165 - mae: 3.2793 - val_loss: 11.4683 - val_mae: 2.8791\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15.8617 - mae: 3.2024 - val_loss: 11.2860 - val_mae: 2.8570\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16.1898 - mae: 3.2298 - val_loss: 11.0883 - val_mae: 2.8327\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.9268 - mae: 3.2118 - val_loss: 10.8997 - val_mae: 2.8093\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.6052 - mae: 3.1944 - val_loss: 10.7408 - val_mae: 2.7891\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.3501 - mae: 3.1409 - val_loss: 10.5630 - val_mae: 2.7665\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.2843 - mae: 3.1392 - val_loss: 10.3910 - val_mae: 2.7439\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.7996 - mae: 3.0765 - val_loss: 10.2300 - val_mae: 2.7226\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.3962 - mae: 3.0674 - val_loss: 10.0737 - val_mae: 2.7014\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.6928 - mae: 3.0731 - val_loss: 9.9204 - val_mae: 2.6804\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.2533 - mae: 3.0340 - val_loss: 9.7688 - val_mae: 2.6597\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.4891 - mae: 3.0633 - val_loss: 9.6344 - val_mae: 2.6405\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 14.2732 - mae: 3.0517 - val_loss: 9.4935 - val_mae: 2.6212\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.9684 - mae: 2.9722 - val_loss: 9.3613 - val_mae: 2.6026\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.8316 - mae: 3.0149 - val_loss: 9.2463 - val_mae: 2.5858\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.9330 - mae: 2.9894 - val_loss: 9.1258 - val_mae: 2.5687\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13.6553 - mae: 2.9847 - val_loss: 9.0068 - val_mae: 2.5503\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13.9797 - mae: 2.9860 - val_loss: 8.8868 - val_mae: 2.5332\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.4751 - mae: 2.9401 - val_loss: 8.7597 - val_mae: 2.5147\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.5515 - mae: 2.9664 - val_loss: 8.6456 - val_mae: 2.4970\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 39ms/step - loss: 42.7784 - mae: 5.2596 - val_loss: 52.8812 - val_mae: 6.4761\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 41.7287 - mae: 5.2105 - val_loss: 50.6266 - val_mae: 6.3524\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 41.3642 - mae: 5.0765 - val_loss: 48.5889 - val_mae: 6.2372\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 40.7292 - mae: 5.0908 - val_loss: 46.7344 - val_mae: 6.1277\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 39.1240 - mae: 5.0102 - val_loss: 44.9847 - val_mae: 6.0211\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 38.8033 - mae: 4.9749 - val_loss: 43.3509 - val_mae: 5.9183\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 38.2945 - mae: 5.0269 - val_loss: 41.9123 - val_mae: 5.8257\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 37.3777 - mae: 4.9183 - val_loss: 40.4680 - val_mae: 5.7297\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 36.4562 - mae: 4.8167 - val_loss: 39.1407 - val_mae: 5.6390\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35.8559 - mae: 4.7668 - val_loss: 37.9206 - val_mae: 5.5536\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 35.3356 - mae: 4.7720 - val_loss: 36.8578 - val_mae: 5.4779\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 35.2400 - mae: 4.7490 - val_loss: 35.7684 - val_mae: 5.3983\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 33.4373 - mae: 4.6279 - val_loss: 34.8582 - val_mae: 5.3306\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33.0206 - mae: 4.6428 - val_loss: 33.8527 - val_mae: 5.2542\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32.7807 - mae: 4.6198 - val_loss: 32.9109 - val_mae: 5.1814\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 32.3211 - mae: 4.5125 - val_loss: 32.0600 - val_mae: 5.1142\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.3036 - mae: 4.5897 - val_loss: 31.1940 - val_mae: 5.0447\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31.0078 - mae: 4.4664 - val_loss: 30.3770 - val_mae: 4.9781\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.9987 - mae: 4.4332 - val_loss: 29.5952 - val_mae: 4.9134\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29.9263 - mae: 4.3903 - val_loss: 28.8612 - val_mae: 4.8515\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 29.5200 - mae: 4.3159 - val_loss: 28.1674 - val_mae: 4.7923\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.9029 - mae: 4.3340 - val_loss: 27.4911 - val_mae: 4.7338\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28.4130 - mae: 4.2622 - val_loss: 26.8100 - val_mae: 4.6739\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 27.7747 - mae: 4.2372 - val_loss: 26.1580 - val_mae: 4.6158\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 27.5969 - mae: 4.2205 - val_loss: 25.5448 - val_mae: 4.5603\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26.9220 - mae: 4.1843 - val_loss: 24.9629 - val_mae: 4.5070\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.7166 - mae: 4.1661 - val_loss: 24.4190 - val_mae: 4.4564\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 26.0828 - mae: 4.0989 - val_loss: 23.8415 - val_mae: 4.4021\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25.6656 - mae: 4.1086 - val_loss: 23.3043 - val_mae: 4.3508\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25.4766 - mae: 4.0647 - val_loss: 22.7948 - val_mae: 4.3016\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.6811 - mae: 3.9544 - val_loss: 22.2940 - val_mae: 4.2526\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.4221 - mae: 3.9734 - val_loss: 21.7876 - val_mae: 4.2024\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24.3040 - mae: 3.9580 - val_loss: 21.3350 - val_mae: 4.1569\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 23.8770 - mae: 3.9165 - val_loss: 20.9131 - val_mae: 4.1140\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.2009 - mae: 3.8825 - val_loss: 20.4744 - val_mae: 4.0688\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.3632 - mae: 3.8975 - val_loss: 20.0560 - val_mae: 4.0252\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 22.5655 - mae: 3.8302 - val_loss: 19.6520 - val_mae: 3.9826\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22.0402 - mae: 3.7473 - val_loss: 19.2621 - val_mae: 3.9410\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 21.6465 - mae: 3.7526 - val_loss: 18.8904 - val_mae: 3.9008\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 22.1125 - mae: 3.7792 - val_loss: 18.5519 - val_mae: 3.8642\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 21.2756 - mae: 3.6787 - val_loss: 18.1987 - val_mae: 3.8288\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 20.9330 - mae: 3.6889 - val_loss: 17.8335 - val_mae: 3.7916\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20.4486 - mae: 3.6464 - val_loss: 17.4713 - val_mae: 3.7542\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.5828 - mae: 3.6458 - val_loss: 17.1261 - val_mae: 3.7182\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 20.0168 - mae: 3.5844 - val_loss: 16.8195 - val_mae: 3.6859\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 19.8172 - mae: 3.6030 - val_loss: 16.4985 - val_mae: 3.6515\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 20.0630 - mae: 3.5985 - val_loss: 16.1886 - val_mae: 3.6181\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.1079 - mae: 3.5260 - val_loss: 15.8922 - val_mae: 3.5858\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19.0931 - mae: 3.5110 - val_loss: 15.5957 - val_mae: 3.5531\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.7276 - mae: 3.5144 - val_loss: 15.3103 - val_mae: 3.5213\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 18.5188 - mae: 3.4770 - val_loss: 15.0214 - val_mae: 3.4885\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.3108 - mae: 3.4600 - val_loss: 14.7486 - val_mae: 3.4572\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.0899 - mae: 3.4319 - val_loss: 14.4916 - val_mae: 3.4276\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.4100 - mae: 3.3513 - val_loss: 14.2553 - val_mae: 3.4000\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 17.9784 - mae: 3.4314 - val_loss: 14.0250 - val_mae: 3.3726\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.3098 - mae: 3.3782 - val_loss: 13.7687 - val_mae: 3.3419\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.0542 - mae: 3.2917 - val_loss: 13.5447 - val_mae: 3.3150\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.9921 - mae: 3.3364 - val_loss: 13.3027 - val_mae: 3.2852\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.7568 - mae: 3.2636 - val_loss: 13.0731 - val_mae: 3.2567\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.4136 - mae: 3.2718 - val_loss: 12.8571 - val_mae: 3.2294\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 16.5139 - mae: 3.2727 - val_loss: 12.6322 - val_mae: 3.2008\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.3444 - mae: 3.2465 - val_loss: 12.4218 - val_mae: 3.1737\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15.7021 - mae: 3.1467 - val_loss: 12.2232 - val_mae: 3.1479\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.9433 - mae: 3.1961 - val_loss: 12.0351 - val_mae: 3.1231\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.4689 - mae: 3.2016 - val_loss: 11.8512 - val_mae: 3.0985\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15.6787 - mae: 3.1807 - val_loss: 11.6684 - val_mae: 3.0738\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 15.4516 - mae: 3.1878 - val_loss: 11.4839 - val_mae: 3.0486\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.3211 - mae: 3.1183 - val_loss: 11.3104 - val_mae: 3.0246\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 14.8248 - mae: 3.1125 - val_loss: 11.1388 - val_mae: 3.0006\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.8966 - mae: 3.1149 - val_loss: 10.9652 - val_mae: 2.9760\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.6612 - mae: 3.0983 - val_loss: 10.8023 - val_mae: 2.9526\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.3949 - mae: 3.0411 - val_loss: 10.6461 - val_mae: 2.9300\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.4991 - mae: 3.0620 - val_loss: 10.4901 - val_mae: 2.9071\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.3274 - mae: 3.0515 - val_loss: 10.3285 - val_mae: 2.8830\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.1896 - mae: 3.0075 - val_loss: 10.1894 - val_mae: 2.8621\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.0458 - mae: 2.9993 - val_loss: 10.0387 - val_mae: 2.8392\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.8147 - mae: 2.9861 - val_loss: 9.8989 - val_mae: 2.8176\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.4182 - mae: 2.9367 - val_loss: 9.7565 - val_mae: 2.7954\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.5023 - mae: 2.9554 - val_loss: 9.6169 - val_mae: 2.7734\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.4892 - mae: 2.9330 - val_loss: 9.4798 - val_mae: 2.7515\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 18.8804 - mae: 3.6344 - val_loss: 9.8486 - val_mae: 2.5209\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.6587 - mae: 3.5990 - val_loss: 9.7077 - val_mae: 2.4993\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.4084 - mae: 3.5812 - val_loss: 9.5711 - val_mae: 2.4781\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.9589 - mae: 3.5074 - val_loss: 9.4399 - val_mae: 2.4575\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.9166 - mae: 3.5026 - val_loss: 9.3107 - val_mae: 2.4371\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.6569 - mae: 3.4746 - val_loss: 9.1863 - val_mae: 2.4172\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 17.3739 - mae: 3.4421 - val_loss: 9.0622 - val_mae: 2.3972\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.0406 - mae: 3.4156 - val_loss: 8.9420 - val_mae: 2.3778\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.1185 - mae: 3.4069 - val_loss: 8.8251 - val_mae: 2.3587\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.7051 - mae: 3.3607 - val_loss: 8.7092 - val_mae: 2.3396\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.4124 - mae: 3.3389 - val_loss: 8.5962 - val_mae: 2.3209\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16.2643 - mae: 3.3143 - val_loss: 8.4869 - val_mae: 2.3026\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16.1281 - mae: 3.3007 - val_loss: 8.3771 - val_mae: 2.2841\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.8222 - mae: 3.2626 - val_loss: 8.2690 - val_mae: 2.2658\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.8168 - mae: 3.2571 - val_loss: 8.1625 - val_mae: 2.2476\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.6022 - mae: 3.2344 - val_loss: 8.0623 - val_mae: 2.2302\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.2950 - mae: 3.1971 - val_loss: 7.9623 - val_mae: 2.2137\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.1653 - mae: 3.1784 - val_loss: 7.8641 - val_mae: 2.1977\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.0109 - mae: 3.1578 - val_loss: 7.7669 - val_mae: 2.1821\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.8101 - mae: 3.1394 - val_loss: 7.6700 - val_mae: 2.1665\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.8313 - mae: 3.1262 - val_loss: 7.5741 - val_mae: 2.1511\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.5035 - mae: 3.0875 - val_loss: 7.4853 - val_mae: 2.1365\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.3773 - mae: 3.0782 - val_loss: 7.3959 - val_mae: 2.1214\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.2439 - mae: 3.0578 - val_loss: 7.3064 - val_mae: 2.1065\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.0846 - mae: 3.0414 - val_loss: 7.2219 - val_mae: 2.0924\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13.9176 - mae: 3.0051 - val_loss: 7.1375 - val_mae: 2.0782\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.8651 - mae: 2.9995 - val_loss: 7.0555 - val_mae: 2.0646\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.6839 - mae: 2.9765 - val_loss: 6.9746 - val_mae: 2.0507\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.5780 - mae: 2.9507 - val_loss: 6.8958 - val_mae: 2.0375\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13.4257 - mae: 2.9429 - val_loss: 6.8156 - val_mae: 2.0267\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13.3398 - mae: 2.9081 - val_loss: 6.7352 - val_mae: 2.0154\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.1624 - mae: 2.9103 - val_loss: 6.6578 - val_mae: 2.0048\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 12.9185 - mae: 2.8671 - val_loss: 6.5829 - val_mae: 1.9945\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 12.8345 - mae: 2.8616 - val_loss: 6.5069 - val_mae: 1.9872\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 12.6586 - mae: 2.8331 - val_loss: 6.4298 - val_mae: 1.9798\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.4205 - mae: 2.7874 - val_loss: 6.3532 - val_mae: 1.9727\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.5144 - mae: 2.8009 - val_loss: 6.2809 - val_mae: 1.9656\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.3887 - mae: 2.7904 - val_loss: 6.2075 - val_mae: 1.9585\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.2590 - mae: 2.7647 - val_loss: 6.1388 - val_mae: 1.9515\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.1734 - mae: 2.7574 - val_loss: 6.0707 - val_mae: 1.9448\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.0511 - mae: 2.7336 - val_loss: 6.0030 - val_mae: 1.9378\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.9128 - mae: 2.7235 - val_loss: 5.9339 - val_mae: 1.9308\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.8115 - mae: 2.6989 - val_loss: 5.8689 - val_mae: 1.9241\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11.7313 - mae: 2.6979 - val_loss: 5.8037 - val_mae: 1.9172\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.6093 - mae: 2.6749 - val_loss: 5.7400 - val_mae: 1.9106\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.5655 - mae: 2.6603 - val_loss: 5.6793 - val_mae: 1.9040\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.3861 - mae: 2.6285 - val_loss: 5.6152 - val_mae: 1.8971\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.3821 - mae: 2.6375 - val_loss: 5.5542 - val_mae: 1.8904\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11.2398 - mae: 2.6178 - val_loss: 5.4951 - val_mae: 1.8840\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.1438 - mae: 2.6072 - val_loss: 5.4355 - val_mae: 1.8774\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.0288 - mae: 2.5882 - val_loss: 5.3783 - val_mae: 1.8709\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10.9430 - mae: 2.5711 - val_loss: 5.3209 - val_mae: 1.8645\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10.8898 - mae: 2.5638 - val_loss: 5.2666 - val_mae: 1.8584\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 10.7752 - mae: 2.5458 - val_loss: 5.2127 - val_mae: 1.8521\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10.7265 - mae: 2.5388 - val_loss: 5.1583 - val_mae: 1.8457\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10.6224 - mae: 2.5233 - val_loss: 5.1059 - val_mae: 1.8394\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10.5291 - mae: 2.5134 - val_loss: 5.0562 - val_mae: 1.8333\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.4238 - mae: 2.4986 - val_loss: 5.0070 - val_mae: 1.8271\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10.3140 - mae: 2.4781 - val_loss: 4.9588 - val_mae: 1.8211\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.2810 - mae: 2.4731 - val_loss: 4.9104 - val_mae: 1.8149\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 10.1996 - mae: 2.4584 - val_loss: 4.8633 - val_mae: 1.8090\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.1158 - mae: 2.4489 - val_loss: 4.8162 - val_mae: 1.8028\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10.0647 - mae: 2.4378 - val_loss: 4.7728 - val_mae: 1.7971\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.9480 - mae: 2.4244 - val_loss: 4.7301 - val_mae: 1.7912\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.8985 - mae: 2.4209 - val_loss: 4.6873 - val_mae: 1.7853\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.8199 - mae: 2.3994 - val_loss: 4.6463 - val_mae: 1.7795\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.7421 - mae: 2.3961 - val_loss: 4.6055 - val_mae: 1.7738\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.7375 - mae: 2.4018 - val_loss: 4.5656 - val_mae: 1.7680\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.5959 - mae: 2.3750 - val_loss: 4.5265 - val_mae: 1.7622\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.5564 - mae: 2.3653 - val_loss: 4.4896 - val_mae: 1.7566\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.4169 - mae: 2.3580 - val_loss: 4.4529 - val_mae: 1.7510\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.4352 - mae: 2.3496 - val_loss: 4.4158 - val_mae: 1.7452\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.3040 - mae: 2.3266 - val_loss: 4.3798 - val_mae: 1.7395\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.2577 - mae: 2.3179 - val_loss: 4.3455 - val_mae: 1.7339\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.2220 - mae: 2.3145 - val_loss: 4.3122 - val_mae: 1.7284\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1427 - mae: 2.3008 - val_loss: 4.2804 - val_mae: 1.7230\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0146 - mae: 2.2870 - val_loss: 4.2493 - val_mae: 1.7176\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0095 - mae: 2.2809 - val_loss: 4.2176 - val_mae: 1.7120\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 9.0064 - mae: 2.2838 - val_loss: 4.1881 - val_mae: 1.7070\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8.9660 - mae: 2.2758 - val_loss: 4.1596 - val_mae: 1.7037\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 44ms/step - loss: 32.4308 - mae: 4.5439 - val_loss: 35.1070 - val_mae: 4.8732\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 31.2842 - mae: 4.4695 - val_loss: 33.9353 - val_mae: 4.7724\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.6885 - mae: 4.4931 - val_loss: 32.8344 - val_mae: 4.6747\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30.3361 - mae: 4.4311 - val_loss: 31.8225 - val_mae: 4.5823\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 30.0404 - mae: 4.4213 - val_loss: 30.8398 - val_mae: 4.5007\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 29.4842 - mae: 4.3671 - val_loss: 29.9031 - val_mae: 4.4293\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 28.5940 - mae: 4.3359 - val_loss: 29.0294 - val_mae: 4.3698\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 27.8520 - mae: 4.2606 - val_loss: 28.1905 - val_mae: 4.3149\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 27.7793 - mae: 4.2376 - val_loss: 27.4017 - val_mae: 4.2700\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27.3326 - mae: 4.2344 - val_loss: 26.6693 - val_mae: 4.2270\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 26.6977 - mae: 4.1586 - val_loss: 25.9505 - val_mae: 4.1829\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 26.6566 - mae: 4.1299 - val_loss: 25.2903 - val_mae: 4.1525\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.6434 - mae: 4.0720 - val_loss: 24.6348 - val_mae: 4.1205\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.0434 - mae: 3.9904 - val_loss: 24.0327 - val_mae: 4.0903\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 24.8781 - mae: 3.9970 - val_loss: 23.4690 - val_mae: 4.0604\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.9517 - mae: 3.8943 - val_loss: 22.9128 - val_mae: 4.0298\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.1386 - mae: 3.9719 - val_loss: 22.3967 - val_mae: 4.0007\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.6909 - mae: 3.8859 - val_loss: 21.8794 - val_mae: 3.9698\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23.0436 - mae: 3.9089 - val_loss: 21.4243 - val_mae: 3.9422\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22.3428 - mae: 3.8270 - val_loss: 20.9633 - val_mae: 3.9128\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22.2905 - mae: 3.7953 - val_loss: 20.5200 - val_mae: 3.8831\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.5630 - mae: 3.7405 - val_loss: 20.1161 - val_mae: 3.8556\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.6346 - mae: 3.7348 - val_loss: 19.6834 - val_mae: 3.8239\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 21.0137 - mae: 3.6937 - val_loss: 19.2811 - val_mae: 3.7941\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 20.7641 - mae: 3.6474 - val_loss: 18.8904 - val_mae: 3.7638\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.0855 - mae: 3.5750 - val_loss: 18.5132 - val_mae: 3.7340\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.0888 - mae: 3.5972 - val_loss: 18.1573 - val_mae: 3.7050\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.5207 - mae: 3.5162 - val_loss: 17.8261 - val_mae: 3.6774\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19.0230 - mae: 3.5015 - val_loss: 17.5048 - val_mae: 3.6497\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 19.3174 - mae: 3.4799 - val_loss: 17.1881 - val_mae: 3.6219\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 18.6569 - mae: 3.4208 - val_loss: 16.9050 - val_mae: 3.5966\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18.4325 - mae: 3.4520 - val_loss: 16.6099 - val_mae: 3.5693\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17.7380 - mae: 3.3759 - val_loss: 16.3496 - val_mae: 3.5454\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.8717 - mae: 3.3485 - val_loss: 16.0797 - val_mae: 3.5195\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.6374 - mae: 3.3659 - val_loss: 15.7968 - val_mae: 3.4910\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.1851 - mae: 3.3348 - val_loss: 15.5319 - val_mae: 3.4637\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.8667 - mae: 3.2683 - val_loss: 15.2886 - val_mae: 3.4391\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.7806 - mae: 3.2376 - val_loss: 15.0623 - val_mae: 3.4152\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.1540 - mae: 3.2223 - val_loss: 14.8203 - val_mae: 3.3886\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.3744 - mae: 3.2583 - val_loss: 14.5894 - val_mae: 3.3630\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16.3869 - mae: 3.2212 - val_loss: 14.3692 - val_mae: 3.3380\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.8694 - mae: 3.1180 - val_loss: 14.1646 - val_mae: 3.3147\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.7133 - mae: 3.1847 - val_loss: 13.9356 - val_mae: 3.2874\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.6875 - mae: 3.1403 - val_loss: 13.7234 - val_mae: 3.2621\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.3481 - mae: 3.1409 - val_loss: 13.5215 - val_mae: 3.2373\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.1427 - mae: 3.1023 - val_loss: 13.3109 - val_mae: 3.2110\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.8105 - mae: 3.0926 - val_loss: 13.1078 - val_mae: 3.1852\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.8700 - mae: 3.1003 - val_loss: 12.9054 - val_mae: 3.1591\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.5522 - mae: 3.0296 - val_loss: 12.7256 - val_mae: 3.1356\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.0767 - mae: 2.9948 - val_loss: 12.5505 - val_mae: 3.1123\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.0495 - mae: 2.9278 - val_loss: 12.3729 - val_mae: 3.0885\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.0071 - mae: 2.9753 - val_loss: 12.1951 - val_mae: 3.0641\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13.8240 - mae: 2.9696 - val_loss: 12.0143 - val_mae: 3.0390\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.6574 - mae: 2.9933 - val_loss: 11.8487 - val_mae: 3.0156\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.4735 - mae: 2.9310 - val_loss: 11.6914 - val_mae: 2.9931\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13.3128 - mae: 2.9045 - val_loss: 11.5350 - val_mae: 2.9704\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.3110 - mae: 2.9001 - val_loss: 11.3841 - val_mae: 2.9482\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.5359 - mae: 2.8931 - val_loss: 11.2368 - val_mae: 2.9263\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.9103 - mae: 2.8503 - val_loss: 11.0774 - val_mae: 2.9022\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.9792 - mae: 2.8667 - val_loss: 10.9265 - val_mae: 2.8791\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12.5170 - mae: 2.8228 - val_loss: 10.7902 - val_mae: 2.8580\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.2348 - mae: 2.7594 - val_loss: 10.6649 - val_mae: 2.8383\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12.4692 - mae: 2.7923 - val_loss: 10.5214 - val_mae: 2.8155\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.9506 - mae: 2.7594 - val_loss: 10.3905 - val_mae: 2.7944\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12.1258 - mae: 2.7755 - val_loss: 10.2608 - val_mae: 2.7733\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.9444 - mae: 2.7218 - val_loss: 10.1458 - val_mae: 2.7543\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.9169 - mae: 2.7258 - val_loss: 10.0161 - val_mae: 2.7327\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.6649 - mae: 2.6907 - val_loss: 9.8972 - val_mae: 2.7126\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.6692 - mae: 2.7041 - val_loss: 9.7814 - val_mae: 2.6929\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.5767 - mae: 2.6725 - val_loss: 9.6610 - val_mae: 2.6722\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11.5226 - mae: 2.6734 - val_loss: 9.5474 - val_mae: 2.6523\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.2681 - mae: 2.6977 - val_loss: 9.4395 - val_mae: 2.6333\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.1577 - mae: 2.6678 - val_loss: 9.3267 - val_mae: 2.6166\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.2177 - mae: 2.6702 - val_loss: 9.2095 - val_mae: 2.5992\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.9497 - mae: 2.6201 - val_loss: 9.1136 - val_mae: 2.5849\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.7773 - mae: 2.5941 - val_loss: 9.0166 - val_mae: 2.5702\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.7341 - mae: 2.6009 - val_loss: 8.9233 - val_mae: 2.5559\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10.5619 - mae: 2.5709 - val_loss: 8.8238 - val_mae: 2.5404\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.6890 - mae: 2.5544 - val_loss: 8.7282 - val_mae: 2.5254\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.6882 - mae: 2.5723 - val_loss: 8.6291 - val_mae: 2.5096\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 47ms/step - loss: 48.0367 - mae: 5.8008 - val_loss: 37.3600 - val_mae: 5.1782\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 47.1732 - mae: 5.7600 - val_loss: 35.0960 - val_mae: 4.9988\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 45.7434 - mae: 5.6704 - val_loss: 33.0474 - val_mae: 4.8361\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 45.4167 - mae: 5.6653 - val_loss: 31.2919 - val_mae: 4.7047\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 44.3265 - mae: 5.5491 - val_loss: 29.7474 - val_mae: 4.5861\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 43.7455 - mae: 5.5560 - val_loss: 28.3370 - val_mae: 4.4702\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 42.2586 - mae: 5.4053 - val_loss: 27.1024 - val_mae: 4.3622\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.4296 - mae: 5.4348 - val_loss: 26.0068 - val_mae: 4.2772\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41.0485 - mae: 5.3683 - val_loss: 25.0183 - val_mae: 4.1951\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 40.1940 - mae: 5.3164 - val_loss: 24.1232 - val_mae: 4.1155\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 39.5677 - mae: 5.2533 - val_loss: 23.3382 - val_mae: 4.0411\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 38.6813 - mae: 5.2132 - val_loss: 22.6415 - val_mae: 3.9711\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 38.1128 - mae: 5.1735 - val_loss: 22.0326 - val_mae: 3.9061\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 36.6498 - mae: 5.0678 - val_loss: 21.4989 - val_mae: 3.8456\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 36.5354 - mae: 5.0911 - val_loss: 21.0231 - val_mae: 3.7893\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 36.1133 - mae: 4.9447 - val_loss: 20.5589 - val_mae: 3.7321\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34.9350 - mae: 4.9491 - val_loss: 20.1025 - val_mae: 3.6741\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 34.0884 - mae: 4.8838 - val_loss: 19.7133 - val_mae: 3.6249\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 33.6209 - mae: 4.8631 - val_loss: 19.3514 - val_mae: 3.5823\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33.3037 - mae: 4.7799 - val_loss: 19.0105 - val_mae: 3.5406\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 32.8176 - mae: 4.7810 - val_loss: 18.6795 - val_mae: 3.4988\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32.2549 - mae: 4.7114 - val_loss: 18.3625 - val_mae: 3.4577\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 31.0584 - mae: 4.6643 - val_loss: 18.0655 - val_mae: 3.4180\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30.8324 - mae: 4.5338 - val_loss: 17.8229 - val_mae: 3.3844\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 30.8028 - mae: 4.5335 - val_loss: 17.5901 - val_mae: 3.3527\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29.6611 - mae: 4.5226 - val_loss: 17.3463 - val_mae: 3.3190\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 29.2444 - mae: 4.5327 - val_loss: 17.1068 - val_mae: 3.2852\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28.8073 - mae: 4.4666 - val_loss: 16.8686 - val_mae: 3.2519\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 28.5399 - mae: 4.4145 - val_loss: 16.6278 - val_mae: 3.2221\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28.2138 - mae: 4.3973 - val_loss: 16.4122 - val_mae: 3.1949\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 27.4417 - mae: 4.2919 - val_loss: 16.1933 - val_mae: 3.1672\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27.1743 - mae: 4.3280 - val_loss: 15.9667 - val_mae: 3.1380\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26.7115 - mae: 4.2503 - val_loss: 15.7545 - val_mae: 3.1104\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25.9083 - mae: 4.1446 - val_loss: 15.5589 - val_mae: 3.0850\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25.2019 - mae: 4.1052 - val_loss: 15.3580 - val_mae: 3.0587\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24.2103 - mae: 4.1215 - val_loss: 15.1758 - val_mae: 3.0348\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26.0914 - mae: 4.2555 - val_loss: 15.0127 - val_mae: 3.0138\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24.8772 - mae: 4.1596 - val_loss: 14.8431 - val_mae: 2.9915\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 23.9338 - mae: 4.0190 - val_loss: 14.6515 - val_mae: 2.9661\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23.9608 - mae: 4.0847 - val_loss: 14.4608 - val_mae: 2.9405\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 23.6680 - mae: 3.9752 - val_loss: 14.2773 - val_mae: 2.9162\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 23.1729 - mae: 3.9240 - val_loss: 14.1020 - val_mae: 2.8932\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22.5922 - mae: 3.9290 - val_loss: 13.9156 - val_mae: 2.8705\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22.6237 - mae: 3.9310 - val_loss: 13.7257 - val_mae: 2.8472\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21.8377 - mae: 3.8245 - val_loss: 13.5435 - val_mae: 2.8246\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 21.7377 - mae: 3.8529 - val_loss: 13.3729 - val_mae: 2.8032\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.7399 - mae: 3.7627 - val_loss: 13.2132 - val_mae: 2.7830\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.8715 - mae: 3.7577 - val_loss: 13.0448 - val_mae: 2.7616\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.6146 - mae: 3.7820 - val_loss: 12.8738 - val_mae: 2.7396\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20.5797 - mae: 3.7292 - val_loss: 12.7317 - val_mae: 2.7211\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20.1429 - mae: 3.6795 - val_loss: 12.5760 - val_mae: 2.7008\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19.9289 - mae: 3.6610 - val_loss: 12.4175 - val_mae: 2.6798\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19.8124 - mae: 3.6555 - val_loss: 12.2604 - val_mae: 2.6589\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 19.6857 - mae: 3.6222 - val_loss: 12.1273 - val_mae: 2.6410\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19.5313 - mae: 3.5946 - val_loss: 11.9816 - val_mae: 2.6213\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 18.9658 - mae: 3.5517 - val_loss: 11.8477 - val_mae: 2.6029\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 18.4539 - mae: 3.5192 - val_loss: 11.7314 - val_mae: 2.5868\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.4333 - mae: 3.4935 - val_loss: 11.5895 - val_mae: 2.5671\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 18.4966 - mae: 3.4794 - val_loss: 11.4615 - val_mae: 2.5492\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.7983 - mae: 3.4778 - val_loss: 11.3226 - val_mae: 2.5296\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17.7091 - mae: 3.4479 - val_loss: 11.1875 - val_mae: 2.5102\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.5999 - mae: 3.3792 - val_loss: 11.0508 - val_mae: 2.4905\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 17.1921 - mae: 3.3422 - val_loss: 10.9281 - val_mae: 2.4727\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16.9685 - mae: 3.3301 - val_loss: 10.8062 - val_mae: 2.4549\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 16.7564 - mae: 3.3468 - val_loss: 10.6908 - val_mae: 2.4377\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16.7333 - mae: 3.3433 - val_loss: 10.5863 - val_mae: 2.4241\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.2678 - mae: 3.2650 - val_loss: 10.4723 - val_mae: 2.4096\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.2228 - mae: 3.2835 - val_loss: 10.3598 - val_mae: 2.3948\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.8334 - mae: 3.2402 - val_loss: 10.2445 - val_mae: 2.3796\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16.1833 - mae: 3.2296 - val_loss: 10.1455 - val_mae: 2.3667\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.7195 - mae: 3.1960 - val_loss: 10.0422 - val_mae: 2.3530\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.4879 - mae: 3.1241 - val_loss: 9.9532 - val_mae: 2.3413\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 15.2345 - mae: 3.1006 - val_loss: 9.8606 - val_mae: 2.3289\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 15.1018 - mae: 3.1108 - val_loss: 9.7651 - val_mae: 2.3159\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.9799 - mae: 3.1163 - val_loss: 9.6610 - val_mae: 2.3014\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.7180 - mae: 3.1195 - val_loss: 9.5607 - val_mae: 2.2874\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.6960 - mae: 3.0852 - val_loss: 9.4592 - val_mae: 2.2730\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.4955 - mae: 3.0780 - val_loss: 9.3664 - val_mae: 2.2598\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.2797 - mae: 3.0449 - val_loss: 9.2729 - val_mae: 2.2464\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.5584 - mae: 3.0869 - val_loss: 9.1852 - val_mae: 2.2349\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 19.0669 - mae: 3.5817 - val_loss: 8.5593 - val_mae: 2.5591\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 18.6904 - mae: 3.5537 - val_loss: 8.4922 - val_mae: 2.5465\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 18.6052 - mae: 3.5338 - val_loss: 8.4248 - val_mae: 2.5338\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 18.1925 - mae: 3.5003 - val_loss: 8.3536 - val_mae: 2.5201\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 17.8276 - mae: 3.4574 - val_loss: 8.2788 - val_mae: 2.5060\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.5710 - mae: 3.4364 - val_loss: 8.2034 - val_mae: 2.4917\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.2784 - mae: 3.4058 - val_loss: 8.1274 - val_mae: 2.4773\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 16.9505 - mae: 3.3730 - val_loss: 8.0490 - val_mae: 2.4625\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16.9338 - mae: 3.3601 - val_loss: 7.9721 - val_mae: 2.4477\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.6528 - mae: 3.3355 - val_loss: 7.8933 - val_mae: 2.4328\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 16.5309 - mae: 3.2999 - val_loss: 7.8138 - val_mae: 2.4173\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16.2378 - mae: 3.2816 - val_loss: 7.7378 - val_mae: 2.4021\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.9682 - mae: 3.2516 - val_loss: 7.6574 - val_mae: 2.3863\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 15.8659 - mae: 3.2420 - val_loss: 7.5790 - val_mae: 2.3707\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.6860 - mae: 3.2090 - val_loss: 7.4999 - val_mae: 2.3548\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15.5566 - mae: 3.1860 - val_loss: 7.4166 - val_mae: 2.3381\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.2352 - mae: 3.1528 - val_loss: 7.3365 - val_mae: 2.3219\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.9667 - mae: 3.1292 - val_loss: 7.2552 - val_mae: 2.3056\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.9159 - mae: 3.1180 - val_loss: 7.1711 - val_mae: 2.2887\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14.7441 - mae: 3.0939 - val_loss: 7.0915 - val_mae: 2.2722\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.6177 - mae: 3.0723 - val_loss: 7.0111 - val_mae: 2.2556\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.4382 - mae: 3.0447 - val_loss: 6.9315 - val_mae: 2.2396\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.3859 - mae: 3.0451 - val_loss: 6.8521 - val_mae: 2.2254\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 14.2479 - mae: 3.0170 - val_loss: 6.7762 - val_mae: 2.2117\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.0788 - mae: 2.9962 - val_loss: 6.6973 - val_mae: 2.1972\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.8457 - mae: 2.9738 - val_loss: 6.6227 - val_mae: 2.1835\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.6804 - mae: 2.9473 - val_loss: 6.5472 - val_mae: 2.1695\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.6140 - mae: 2.9441 - val_loss: 6.4718 - val_mae: 2.1557\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.4115 - mae: 2.9139 - val_loss: 6.3945 - val_mae: 2.1415\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 13.2762 - mae: 2.8954 - val_loss: 6.3181 - val_mae: 2.1273\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.1371 - mae: 2.8849 - val_loss: 6.2423 - val_mae: 2.1131\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.0473 - mae: 2.8565 - val_loss: 6.1684 - val_mae: 2.0990\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12.8775 - mae: 2.8470 - val_loss: 6.0941 - val_mae: 2.0850\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.8851 - mae: 2.8358 - val_loss: 6.0222 - val_mae: 2.0712\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 12.6554 - mae: 2.8058 - val_loss: 5.9497 - val_mae: 2.0570\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.4771 - mae: 2.7900 - val_loss: 5.8793 - val_mae: 2.0433\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.3774 - mae: 2.7606 - val_loss: 5.8090 - val_mae: 2.0294\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.3163 - mae: 2.7600 - val_loss: 5.7392 - val_mae: 2.0156\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12.1863 - mae: 2.7403 - val_loss: 5.6686 - val_mae: 2.0013\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.0232 - mae: 2.7114 - val_loss: 5.5996 - val_mae: 1.9874\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.9503 - mae: 2.7096 - val_loss: 5.5303 - val_mae: 1.9734\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.8566 - mae: 2.6849 - val_loss: 5.4637 - val_mae: 1.9596\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.6408 - mae: 2.6687 - val_loss: 5.3976 - val_mae: 1.9459\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.5465 - mae: 2.6611 - val_loss: 5.3293 - val_mae: 1.9319\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.4773 - mae: 2.6412 - val_loss: 5.2634 - val_mae: 1.9183\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.4466 - mae: 2.6260 - val_loss: 5.1973 - val_mae: 1.9046\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11.3139 - mae: 2.6053 - val_loss: 5.1323 - val_mae: 1.8909\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.0982 - mae: 2.5845 - val_loss: 5.0686 - val_mae: 1.8776\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.0779 - mae: 2.5731 - val_loss: 5.0058 - val_mae: 1.8640\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11.0156 - mae: 2.5721 - val_loss: 4.9449 - val_mae: 1.8508\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.8738 - mae: 2.5546 - val_loss: 4.8858 - val_mae: 1.8374\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10.8163 - mae: 2.5387 - val_loss: 4.8261 - val_mae: 1.8239\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.8115 - mae: 2.5347 - val_loss: 4.7664 - val_mae: 1.8102\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.6479 - mae: 2.5081 - val_loss: 4.7092 - val_mae: 1.7967\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.6108 - mae: 2.4977 - val_loss: 4.6526 - val_mae: 1.7831\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.5267 - mae: 2.4844 - val_loss: 4.5968 - val_mae: 1.7694\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.4125 - mae: 2.4715 - val_loss: 4.5426 - val_mae: 1.7561\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.3211 - mae: 2.4644 - val_loss: 4.4881 - val_mae: 1.7427\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10.2193 - mae: 2.4503 - val_loss: 4.4362 - val_mae: 1.7296\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.1967 - mae: 2.4333 - val_loss: 4.3860 - val_mae: 1.7185\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.1095 - mae: 2.4241 - val_loss: 4.3379 - val_mae: 1.7082\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.9567 - mae: 2.4064 - val_loss: 4.2901 - val_mae: 1.6992\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.9191 - mae: 2.3999 - val_loss: 4.2418 - val_mae: 1.6910\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.9098 - mae: 2.3963 - val_loss: 4.1968 - val_mae: 1.6831\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.7540 - mae: 2.3752 - val_loss: 4.1509 - val_mae: 1.6751\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.6988 - mae: 2.3649 - val_loss: 4.1060 - val_mae: 1.6670\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.5986 - mae: 2.3421 - val_loss: 4.0610 - val_mae: 1.6589\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.6050 - mae: 2.3538 - val_loss: 4.0175 - val_mae: 1.6509\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4892 - mae: 2.3289 - val_loss: 3.9724 - val_mae: 1.6425\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.4165 - mae: 2.3234 - val_loss: 3.9304 - val_mae: 1.6345\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.3466 - mae: 2.3100 - val_loss: 3.8883 - val_mae: 1.6263\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.2109 - mae: 2.2991 - val_loss: 3.8482 - val_mae: 1.6183\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.3255 - mae: 2.3003 - val_loss: 3.8077 - val_mae: 1.6122\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.1931 - mae: 2.2998 - val_loss: 3.7703 - val_mae: 1.6068\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0687 - mae: 2.2755 - val_loss: 3.7340 - val_mae: 1.6014\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0501 - mae: 2.2736 - val_loss: 3.6984 - val_mae: 1.5959\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.9355 - mae: 2.2427 - val_loss: 3.6650 - val_mae: 1.5907\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.9842 - mae: 2.2579 - val_loss: 3.6307 - val_mae: 1.5852\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.8460 - mae: 2.2444 - val_loss: 3.5984 - val_mae: 1.5800\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.8220 - mae: 2.2416 - val_loss: 3.5662 - val_mae: 1.5748\n",
      "Epoch 1/80\n",
      "5/5 [==============================] - 1s 42ms/step - loss: 18.0858 - mae: 3.5186 - val_loss: 9.1682 - val_mae: 2.5722\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.8888 - mae: 3.4956 - val_loss: 9.1485 - val_mae: 2.5651\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.6557 - mae: 3.4691 - val_loss: 9.1222 - val_mae: 2.5567\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17.3961 - mae: 3.4430 - val_loss: 9.0928 - val_mae: 2.5476\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17.2698 - mae: 3.4222 - val_loss: 9.0563 - val_mae: 2.5373\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.0033 - mae: 3.4013 - val_loss: 9.0126 - val_mae: 2.5258\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.8511 - mae: 3.3814 - val_loss: 8.9642 - val_mae: 2.5134\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.6639 - mae: 3.3548 - val_loss: 8.9094 - val_mae: 2.4999\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.4698 - mae: 3.3335 - val_loss: 8.8539 - val_mae: 2.4861\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 16.2979 - mae: 3.3107 - val_loss: 8.7912 - val_mae: 2.4713\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 16.1496 - mae: 3.2944 - val_loss: 8.7285 - val_mae: 2.4563\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.9545 - mae: 3.2709 - val_loss: 8.6588 - val_mae: 2.4404\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.7954 - mae: 3.2460 - val_loss: 8.5901 - val_mae: 2.4241\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.6074 - mae: 3.2265 - val_loss: 8.5166 - val_mae: 2.4071\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.4224 - mae: 3.2040 - val_loss: 8.4375 - val_mae: 2.3898\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15.2977 - mae: 3.1851 - val_loss: 8.3580 - val_mae: 2.3720\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.1471 - mae: 3.1619 - val_loss: 8.2780 - val_mae: 2.3541\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 15.0097 - mae: 3.1445 - val_loss: 8.1982 - val_mae: 2.3360\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.8098 - mae: 3.1218 - val_loss: 8.1186 - val_mae: 2.3180\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14.6985 - mae: 3.1028 - val_loss: 8.0344 - val_mae: 2.3012\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.5658 - mae: 3.0854 - val_loss: 7.9494 - val_mae: 2.2852\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 14.4056 - mae: 3.0673 - val_loss: 7.8648 - val_mae: 2.2689\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.2834 - mae: 3.0463 - val_loss: 7.7794 - val_mae: 2.2524\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.1224 - mae: 3.0261 - val_loss: 7.6912 - val_mae: 2.2360\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.0316 - mae: 3.0102 - val_loss: 7.6084 - val_mae: 2.2196\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 13.8754 - mae: 2.9923 - val_loss: 7.5234 - val_mae: 2.2035\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.7415 - mae: 2.9702 - val_loss: 7.4350 - val_mae: 2.1871\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.6330 - mae: 2.9543 - val_loss: 7.3529 - val_mae: 2.1708\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13.5233 - mae: 2.9375 - val_loss: 7.2679 - val_mae: 2.1541\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.4045 - mae: 2.9196 - val_loss: 7.1835 - val_mae: 2.1393\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.2625 - mae: 2.9017 - val_loss: 7.1014 - val_mae: 2.1253\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.1699 - mae: 2.8826 - val_loss: 7.0195 - val_mae: 2.1135\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.0440 - mae: 2.8671 - val_loss: 6.9426 - val_mae: 2.1025\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12.9320 - mae: 2.8477 - val_loss: 6.8575 - val_mae: 2.0912\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12.8004 - mae: 2.8325 - val_loss: 6.7820 - val_mae: 2.0804\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.6716 - mae: 2.8117 - val_loss: 6.7021 - val_mae: 2.0694\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12.5938 - mae: 2.8000 - val_loss: 6.6238 - val_mae: 2.0582\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.4813 - mae: 2.7810 - val_loss: 6.5457 - val_mae: 2.0472\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12.3674 - mae: 2.7656 - val_loss: 6.4717 - val_mae: 2.0361\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.2466 - mae: 2.7487 - val_loss: 6.3956 - val_mae: 2.0251\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.1635 - mae: 2.7351 - val_loss: 6.3180 - val_mae: 2.0140\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12.0862 - mae: 2.7219 - val_loss: 6.2457 - val_mae: 2.0032\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.9633 - mae: 2.7036 - val_loss: 6.1731 - val_mae: 1.9922\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.8506 - mae: 2.6871 - val_loss: 6.1006 - val_mae: 1.9812\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.7578 - mae: 2.6736 - val_loss: 6.0287 - val_mae: 1.9702\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 11.6474 - mae: 2.6570 - val_loss: 5.9575 - val_mae: 1.9595\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.5441 - mae: 2.6417 - val_loss: 5.8866 - val_mae: 1.9490\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.4707 - mae: 2.6281 - val_loss: 5.8176 - val_mae: 1.9385\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.3778 - mae: 2.6153 - val_loss: 5.7510 - val_mae: 1.9278\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.2839 - mae: 2.6012 - val_loss: 5.6840 - val_mae: 1.9171\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11.1935 - mae: 2.5875 - val_loss: 5.6165 - val_mae: 1.9064\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.0968 - mae: 2.5713 - val_loss: 5.5504 - val_mae: 1.8959\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.0016 - mae: 2.5582 - val_loss: 5.4837 - val_mae: 1.8854\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.9230 - mae: 2.5432 - val_loss: 5.4190 - val_mae: 1.8751\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.8414 - mae: 2.5355 - val_loss: 5.3580 - val_mae: 1.8648\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.7530 - mae: 2.5192 - val_loss: 5.2945 - val_mae: 1.8546\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.6811 - mae: 2.5100 - val_loss: 5.2321 - val_mae: 1.8441\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.5975 - mae: 2.4970 - val_loss: 5.1749 - val_mae: 1.8339\n",
      "Epoch 59/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.5146 - mae: 2.4849 - val_loss: 5.1166 - val_mae: 1.8235\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.4368 - mae: 2.4745 - val_loss: 5.0601 - val_mae: 1.8151\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.3500 - mae: 2.4624 - val_loss: 5.0046 - val_mae: 1.8072\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.2590 - mae: 2.4500 - val_loss: 4.9480 - val_mae: 1.7992\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10.2062 - mae: 2.4400 - val_loss: 4.8906 - val_mae: 1.7910\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.1109 - mae: 2.4271 - val_loss: 4.8342 - val_mae: 1.7828\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.0656 - mae: 2.4182 - val_loss: 4.7786 - val_mae: 1.7746\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10.0012 - mae: 2.4060 - val_loss: 4.7299 - val_mae: 1.7672\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.9096 - mae: 2.3939 - val_loss: 4.6814 - val_mae: 1.7598\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9.8278 - mae: 2.3850 - val_loss: 4.6305 - val_mae: 1.7520\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.7592 - mae: 2.3750 - val_loss: 4.5801 - val_mae: 1.7442\n",
      "Epoch 70/80\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3992 - mae: 1.7847"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cdiazj\\DeepLearning\\resmol\\notebooks\\6.0-sdj-LassoRegression.ipynb Cell 10\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m tuner\u001b[39m.\u001b[39msearch_space_summary()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Start the search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_data, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m              epochs\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m              validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# After the tuner is finished, retrieve the best model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cdiazj/DeepLearning/resmol/notebooks/6.0-sdj-LassoRegression.ipynb#X13sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models(num_models\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_and_update_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[0;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[0;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[0;32m    238\u001b[0m     ):\n\u001b[0;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    251\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    286\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 287\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_and_fit_model(trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    289\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    213\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 214\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mfit(hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[0;32m    216\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_tuner\\engine\\hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    121\u001b[0m     \u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Layer, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "class LassoLayer(Layer):\n",
    "    def __init__(self, alpha=1.0, **kwargs):\n",
    "        super(LassoLayer, self).__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], 1),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.dot(inputs, self.kernel) + self.alpha\n",
    "\n",
    "def build_model(hp):\n",
    "    # Input layers\n",
    "    input_onehot_1d = Input(shape=(28, 1))\n",
    "\n",
    "    # Flatten layers\n",
    "    flat_onehot_1d = Flatten()(input_onehot_1d)\n",
    "\n",
    "    # Lasso model\n",
    "    alpha = hp.Float('alpha', min_value=0.01, max_value=1.0)\n",
    "    lasso_output = LassoLayer(alpha=alpha)(flat_onehot_1d)\n",
    "\n",
    "    # Scaler layer\n",
    "    scaler_output = BatchNormalization()(lasso_output)\n",
    "\n",
    "    # Output layer\n",
    "    output_1d = Dense(1, activation='linear')(scaler_output)\n",
    "    output_1d = output_1d * 5\n",
    "\n",
    "    # Create model\n",
    "    model_1d = Model(inputs=[input_onehot_1d], outputs=output_1d)\n",
    "\n",
    "    # Compile model\n",
    "    opt = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    model_1d.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    return model_1d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.concatenate((onehot_df_encoded, H_encoded), axis = 1)\n",
    "# assuming onehot_df_train, H_train, y_train are your training datasets\n",
    "input_shape_onehot = (train_data.shape[1], 1)\n",
    "\n",
    "train_data, y_train = shuffle(train_data, y_train)\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=15,\n",
    "    directory='output',\n",
    "    project_name='AirQuality',\n",
    "    overwrite=True)\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Start the search for the best hyperparameters\n",
    "tuner.search(train_data, y_train,\n",
    "             epochs=80,\n",
    "             validation_split=0.1)\n",
    "\n",
    "# After the tuner is finished, retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# And its hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_data, y_train, epochs=150, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Let's assume you have a test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot_df_test_encoded, H_test_encoded = prepare_test_data(onehot_df_test, H_test)  # You need to ensure that your test data is prepared in the same way as your training data\n",
    "test_data = np.concatenate((onehot_df_test_encoded, H_test_encoded), axis = 1)\n",
    "# Now you can make predictions on your test set\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE): ', mse)\n",
    "print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "print('Mean Absolute Error (MAE): ', mae)\n",
    "print('R-squared Score (R^2): ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot predicted vs real values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Real')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "differences = predictions.flatten() - y_test.flatten() # This will give the difference between your predictions and the actual values\n",
    "\n",
    "#Create a figure with two subplots: a histogram of the differences and a scatter plot of predicted vs real values\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(10, 15))\n",
    "\n",
    "#Plot histogram\n",
    "axs[0].hist(differences, bins=20, density=True)\n",
    "axs[0].set_title('Histogram of differences between predicted and actual values')\n",
    "axs[0].set_xlabel('Differences')\n",
    "axs[0].set_ylabel('Density')\n",
    "\n",
    "#Plot scatter\n",
    "axs[1].scatter(y_test, predictions, alpha=0.5)\n",
    "axs[1].set_title('Scatter plot of predicted vs actual values')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "\n",
    "#Draw a diagonal line on the scatterplot\n",
    "lims = [np.min([axs[1].get_xlim(), axs[1].get_ylim()]), # min of both axes\n",
    "np.max([axs[1].get_xlim(), axs[1].get_ylim()])] # max of both axes\n",
    "axs[1].plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "axs[1].set_xlim(lims)\n",
    "axs[1].set_ylim(lims)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
