{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.249085100Z",
     "start_time": "2023-06-13T18:27:55.242080400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fboost import outlier_iqr, DataPreparator, FeatureBoosterRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value\n",
    "seed_value= 0\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.306118300Z",
     "start_time": "2023-06-13T18:27:55.248085500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw/alldata.csv')\n",
    "data.columns = ['familia','indice_fam','subposicion_1','subposicion_2','subposicion_3','subposicion_4', 'energia']\n",
    "mask = (data[['subposicion_1','subposicion_2','subposicion_3','subposicion_4']] == 0).sum(axis=1) > 1\n",
    "lista_train = data.loc[mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]\n",
    "lista_test = data.loc[~mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:27:55.307115700Z",
     "start_time": "2023-06-13T18:27:55.271000700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "X_train = lista_train.drop(['energia'], axis=1)\n",
    "X_test = lista_test.drop(['energia'], axis=1)\n",
    "\n",
    "# Fit the encoder and transform the data for both train and test dataframes\n",
    "encoded_train = encoder.fit_transform(X_train)\n",
    "encoded_test = encoder.transform(X_test)\n",
    "\n",
    "# Now, 'encoded_train' and 'encoded_test' are numpy arrays, we can convert them back to dataframes:\n",
    "X_train_scaled = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(X_train.columns))\n",
    "X_test_scaled = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(X_test.columns))\n",
    "\n",
    "\n",
    "y_test = lista_test['energia']\n",
    "y_train = lista_train['energia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = DataPreparator(outliers_strategy = 'IQR', \n",
    "                           outliers_cutoff = 3, \n",
    "                           encoding_strategy = 'dummy',\n",
    "                           drop_duplicate_rows = True)\n",
    "X_train, y_train = data_prep.fit_transform(X_train, y_train)\n",
    "X_test = data_prep.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 170 entries, 0 to 1269\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   subposicion_1    170 non-null    int64  \n",
      " 1   subposicion_2    170 non-null    int64  \n",
      " 2   subposicion_3    170 non-null    int64  \n",
      " 3   subposicion_4    170 non-null    int64  \n",
      " 4   subposicion_12   170 non-null    float64\n",
      " 5   subposicion_42   170 non-null    float64\n",
      " 6   RULE_EXTRACT_19  170 non-null    float64\n",
      " 7   RULE_EXTRACT_8   170 non-null    float64\n",
      " 8   RULE_EXTRACT_26  170 non-null    float64\n",
      " 9   RULE_EXTRACT_10  170 non-null    float64\n",
      " 10  RULE_EXTRACT_20  170 non-null    float64\n",
      " 11  RULE_EXTRACT_27  170 non-null    float64\n",
      "dtypes: float64(8), int64(4)\n",
      "memory usage: 17.3 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# FEATURE ENGINEERING - 1/2 - Polynomials + Rules extraction\n",
    "fboost = FeatureBoosterRegressor(base_model = RandomForestRegressor(criterion='friedman_mse', \n",
    "                                                    max_depth=5,\n",
    "                                                    max_features=None, \n",
    "                                                    max_leaf_nodes=2,\n",
    "                                                    min_samples_leaf=1, \n",
    "                                                    verbose=0, \n",
    "                                                    n_estimators = 850,\n",
    "                                                    warm_start=True,\n",
    "                                                    random_state = 0),\n",
    "                                max_rules = 2800, \n",
    "                                n_best_rules = 35,\n",
    "                                original_features_selection= False,\n",
    "                                selection_strategy = 'severe',  \n",
    "                                quantile_cutoff = 0.83,\n",
    "                                alpha = 89,\n",
    "                                scaler = 'Standard',\n",
    "                                random_state = 0)\n",
    "\n",
    "# FIT FEATURE ENGINEERING FOR TRAIN DATA\n",
    "X_train, rules = fboost.fit_transform(X_train, y_train)\n",
    "\n",
    "# TRANSFORM FEATURE ENGINEERING FOR TEST DATA\n",
    "X_test = fboost.transform(X_test)\n",
    "\n",
    "#LET HAVE A LOOK AT THE NEW FEATURES WE JUST CREATED\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 12)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Layer, BatchNormalization, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "X_test_array = np.array(X_test)\n",
    "\n",
    "#train_data = X_train_array.reshape((X_train_array.shape[0], 12))\n",
    "X_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "5/5 [==============================] - 1s 37ms/step - loss: 10.3290 - val_loss: 8.4079\n",
      "Epoch 2/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7697 - val_loss: 14.0950\n",
      "Epoch 3/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6299 - val_loss: 7.5376\n",
      "Epoch 4/250\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.1906 - val_loss: 9.9445\n",
      "Epoch 5/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.7118 - val_loss: 9.3810\n",
      "Epoch 6/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4989 - val_loss: 5.0692\n",
      "Epoch 7/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.3087 - val_loss: 6.4338\n",
      "Epoch 8/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.1421 - val_loss: 5.8744\n",
      "Epoch 9/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9796 - val_loss: 6.0884\n",
      "Epoch 10/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8140 - val_loss: 7.2023\n",
      "Epoch 11/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7597 - val_loss: 7.9497\n",
      "Epoch 12/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8256 - val_loss: 7.3441\n",
      "Epoch 13/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6387 - val_loss: 9.1341\n",
      "Epoch 14/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6643 - val_loss: 7.2026\n",
      "Epoch 15/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8529 - val_loss: 9.0176\n",
      "Epoch 16/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8362 - val_loss: 9.4538\n",
      "Epoch 17/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8600 - val_loss: 8.6903\n",
      "Epoch 18/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7505 - val_loss: 10.3936\n",
      "Epoch 19/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6618 - val_loss: 6.1714\n",
      "Epoch 20/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8458 - val_loss: 12.0096\n",
      "Epoch 21/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2059 - val_loss: 7.5233\n",
      "Epoch 22/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8954 - val_loss: 6.5715\n",
      "Epoch 23/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5432 - val_loss: 9.8117\n",
      "Epoch 24/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6118 - val_loss: 8.9524\n",
      "Epoch 25/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5241 - val_loss: 8.9891\n",
      "Epoch 26/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5092 - val_loss: 9.1933\n",
      "Epoch 27/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5318 - val_loss: 8.8210\n",
      "Epoch 28/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5200 - val_loss: 10.3154\n",
      "Epoch 29/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5290 - val_loss: 8.7628\n",
      "Epoch 30/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.7936 - val_loss: 6.9722\n",
      "Epoch 31/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9285 - val_loss: 9.0791\n",
      "Epoch 32/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8186 - val_loss: 6.4891\n",
      "Epoch 33/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6484 - val_loss: 7.2623\n",
      "Epoch 34/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5808 - val_loss: 9.5042\n",
      "Epoch 35/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6223 - val_loss: 6.8914\n",
      "Epoch 36/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3996 - val_loss: 9.9457\n",
      "Epoch 37/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5906 - val_loss: 9.1130\n",
      "Epoch 38/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4929 - val_loss: 7.9409\n",
      "Epoch 39/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4200 - val_loss: 8.0939\n",
      "Epoch 40/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4484 - val_loss: 8.2734\n",
      "Epoch 41/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5581 - val_loss: 7.7578\n",
      "Epoch 42/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4832 - val_loss: 8.8193\n",
      "Epoch 43/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3958 - val_loss: 7.4646\n",
      "Epoch 44/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3508 - val_loss: 9.4111\n",
      "Epoch 45/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4816 - val_loss: 8.1472\n",
      "Epoch 46/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4616 - val_loss: 6.9220\n",
      "Epoch 47/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3857 - val_loss: 11.4084\n",
      "Epoch 48/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6500 - val_loss: 7.3600\n",
      "Epoch 49/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6706 - val_loss: 7.8806\n",
      "Epoch 50/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8219 - val_loss: 11.5562\n",
      "Epoch 51/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8843 - val_loss: 6.3966\n",
      "Epoch 52/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.5471 - val_loss: 8.8239\n",
      "Epoch 53/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4948 - val_loss: 7.7419\n",
      "Epoch 54/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6932 - val_loss: 6.9325\n",
      "Epoch 55/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5108 - val_loss: 9.4914\n",
      "Epoch 56/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5446 - val_loss: 7.1552\n",
      "Epoch 57/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.7704 - val_loss: 6.5480\n",
      "Epoch 58/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5787 - val_loss: 8.4298\n",
      "Epoch 59/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.6679 - val_loss: 6.6769\n",
      "Epoch 60/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4687 - val_loss: 7.4339\n",
      "Epoch 61/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6853 - val_loss: 7.6880\n",
      "Epoch 62/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3980 - val_loss: 8.4413\n",
      "Epoch 63/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5502 - val_loss: 7.3896\n",
      "Epoch 64/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3253 - val_loss: 8.0711\n",
      "Epoch 65/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3886 - val_loss: 8.9676\n",
      "Epoch 66/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3410 - val_loss: 7.3172\n",
      "Epoch 67/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3142 - val_loss: 7.2961\n",
      "Epoch 68/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3104 - val_loss: 8.3283\n",
      "Epoch 69/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3129 - val_loss: 8.6733\n",
      "Epoch 70/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2375 - val_loss: 7.3671\n",
      "Epoch 71/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3652 - val_loss: 7.9093\n",
      "Epoch 72/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2751 - val_loss: 7.8250\n",
      "Epoch 73/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3435 - val_loss: 7.0722\n",
      "Epoch 74/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2925 - val_loss: 7.7110\n",
      "Epoch 75/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2832 - val_loss: 8.0866\n",
      "Epoch 76/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2793 - val_loss: 8.0426\n",
      "Epoch 77/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2281 - val_loss: 7.2858\n",
      "Epoch 78/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3417 - val_loss: 7.8997\n",
      "Epoch 79/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3099 - val_loss: 7.8923\n",
      "Epoch 80/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6064 - val_loss: 8.5201\n",
      "Epoch 81/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4095 - val_loss: 6.1468\n",
      "Epoch 82/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4085 - val_loss: 7.8550\n",
      "Epoch 83/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3584 - val_loss: 7.5571\n",
      "Epoch 84/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2091 - val_loss: 6.9157\n",
      "Epoch 85/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3558 - val_loss: 6.9950\n",
      "Epoch 86/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4103 - val_loss: 7.5017\n",
      "Epoch 87/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3800 - val_loss: 8.5337\n",
      "Epoch 88/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.2821 - val_loss: 6.5899\n",
      "Epoch 89/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2729 - val_loss: 7.9961\n",
      "Epoch 90/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2273 - val_loss: 7.7223\n",
      "Epoch 91/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3759 - val_loss: 6.0290\n",
      "Epoch 92/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3244 - val_loss: 7.6149\n",
      "Epoch 93/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3079 - val_loss: 6.7947\n",
      "Epoch 94/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2592 - val_loss: 6.6621\n",
      "Epoch 95/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1866 - val_loss: 7.0941\n",
      "Epoch 96/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2253 - val_loss: 7.5133\n",
      "Epoch 97/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2076 - val_loss: 7.5650\n",
      "Epoch 98/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2377 - val_loss: 6.7286\n",
      "Epoch 99/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2464 - val_loss: 7.6609\n",
      "Epoch 100/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2827 - val_loss: 7.9785\n",
      "Epoch 101/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2004 - val_loss: 6.3262\n",
      "Epoch 102/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2934 - val_loss: 7.2781\n",
      "Epoch 103/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1949 - val_loss: 7.4768\n",
      "Epoch 104/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1440 - val_loss: 7.0853\n",
      "Epoch 105/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2383 - val_loss: 7.4122\n",
      "Epoch 106/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1122 - val_loss: 6.0176\n",
      "Epoch 107/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2560 - val_loss: 7.3610\n",
      "Epoch 108/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1268 - val_loss: 7.0492\n",
      "Epoch 109/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0927 - val_loss: 6.1400\n",
      "Epoch 110/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1825 - val_loss: 6.1890\n",
      "Epoch 111/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3627 - val_loss: 6.3810\n",
      "Epoch 112/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2446 - val_loss: 7.9203\n",
      "Epoch 113/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1873 - val_loss: 6.4497\n",
      "Epoch 114/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1371 - val_loss: 6.9733\n",
      "Epoch 115/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1514 - val_loss: 7.6603\n",
      "Epoch 116/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2473 - val_loss: 6.8201\n",
      "Epoch 117/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.2805 - val_loss: 6.7069\n",
      "Epoch 118/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.0901 - val_loss: 7.7034\n",
      "Epoch 119/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1606 - val_loss: 6.7300\n",
      "Epoch 120/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1893 - val_loss: 6.5310\n",
      "Epoch 121/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3946 - val_loss: 8.1177\n",
      "Epoch 122/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2513 - val_loss: 6.3083\n",
      "Epoch 123/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1567 - val_loss: 7.4005\n",
      "Epoch 124/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1520 - val_loss: 6.5143\n",
      "Epoch 125/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1189 - val_loss: 6.6239\n",
      "Epoch 126/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0844 - val_loss: 6.7074\n",
      "Epoch 127/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1885 - val_loss: 6.1575\n",
      "Epoch 128/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0848 - val_loss: 7.7139\n",
      "Epoch 129/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1105 - val_loss: 6.7119\n",
      "Epoch 130/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0528 - val_loss: 6.5471\n",
      "Epoch 131/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0939 - val_loss: 6.4316\n",
      "Epoch 132/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0651 - val_loss: 6.4546\n",
      "Epoch 133/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0962 - val_loss: 6.0999\n",
      "Epoch 134/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0733 - val_loss: 6.1772\n",
      "Epoch 135/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.0424 - val_loss: 7.1557\n",
      "Epoch 136/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.2943 - val_loss: 6.9848\n",
      "Epoch 137/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3004 - val_loss: 6.8171\n",
      "Epoch 138/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2817 - val_loss: 7.2325\n",
      "Epoch 139/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2728 - val_loss: 6.0614\n",
      "Epoch 140/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4242 - val_loss: 8.7324\n",
      "Epoch 141/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4948 - val_loss: 6.6135\n",
      "Epoch 142/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3360 - val_loss: 6.8059\n",
      "Epoch 143/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3464 - val_loss: 7.8316\n",
      "Epoch 144/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2540 - val_loss: 5.4381\n",
      "Epoch 145/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3897 - val_loss: 8.0927\n",
      "Epoch 146/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2777 - val_loss: 6.8343\n",
      "Epoch 147/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1533 - val_loss: 7.2015\n",
      "Epoch 148/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1575 - val_loss: 7.3223\n",
      "Epoch 149/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0826 - val_loss: 8.0328\n",
      "Epoch 150/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0899 - val_loss: 7.2240\n",
      "Epoch 151/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3331 - val_loss: 7.5268\n",
      "Epoch 152/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.0753 - val_loss: 6.9405\n",
      "Epoch 153/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2456 - val_loss: 7.2362\n",
      "Epoch 154/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1522 - val_loss: 6.5770\n",
      "Epoch 155/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0321 - val_loss: 6.3179\n",
      "Epoch 156/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0778 - val_loss: 6.7136\n",
      "Epoch 157/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0836 - val_loss: 5.9716\n",
      "Epoch 158/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0265 - val_loss: 5.4209\n",
      "Epoch 159/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0950 - val_loss: 6.8095\n",
      "Epoch 160/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1080 - val_loss: 6.5891\n",
      "Epoch 161/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0149 - val_loss: 5.5756\n",
      "Epoch 162/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0274 - val_loss: 6.2495\n",
      "Epoch 163/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0579 - val_loss: 5.6873\n",
      "Epoch 164/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0800 - val_loss: 5.6450\n",
      "Epoch 165/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0411 - val_loss: 6.8539\n",
      "Epoch 166/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0930 - val_loss: 6.1444\n",
      "Epoch 167/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0225 - val_loss: 5.9957\n",
      "Epoch 168/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0748 - val_loss: 6.0814\n",
      "Epoch 169/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0052 - val_loss: 5.7566\n",
      "Epoch 170/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.9794 - val_loss: 5.3174\n",
      "Epoch 171/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9687 - val_loss: 5.6349\n",
      "Epoch 172/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0290 - val_loss: 5.8435\n",
      "Epoch 173/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9470 - val_loss: 5.1905\n",
      "Epoch 174/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0368 - val_loss: 5.5596\n",
      "Epoch 175/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0646 - val_loss: 5.6620\n",
      "Epoch 176/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0095 - val_loss: 5.3611\n",
      "Epoch 177/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0840 - val_loss: 6.5566\n",
      "Epoch 178/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0700 - val_loss: 6.1322\n",
      "Epoch 179/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0517 - val_loss: 5.6923\n",
      "Epoch 180/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0274 - val_loss: 7.1571\n",
      "Epoch 181/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0278 - val_loss: 5.6043\n",
      "Epoch 182/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0454 - val_loss: 5.2071\n",
      "Epoch 183/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0223 - val_loss: 6.0815\n",
      "Epoch 184/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9926 - val_loss: 5.4522\n",
      "Epoch 185/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0104 - val_loss: 5.2522\n",
      "Epoch 186/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9458 - val_loss: 5.6399\n",
      "Epoch 187/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9489 - val_loss: 5.6077\n",
      "Epoch 188/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9298 - val_loss: 6.1098\n",
      "Epoch 189/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9996 - val_loss: 5.6552\n",
      "Epoch 190/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9838 - val_loss: 5.5287\n",
      "Epoch 191/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8898 - val_loss: 6.8480\n",
      "Epoch 192/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0371 - val_loss: 5.8810\n",
      "Epoch 193/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1487 - val_loss: 5.7648\n",
      "Epoch 194/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0438 - val_loss: 6.4585\n",
      "Epoch 195/250\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.9936 - val_loss: 5.1036\n",
      "Epoch 196/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0669 - val_loss: 5.6759\n",
      "Epoch 197/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0416 - val_loss: 6.5748\n",
      "Epoch 198/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9809 - val_loss: 5.0323\n",
      "Epoch 199/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9975 - val_loss: 6.1598\n",
      "Epoch 200/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2458 - val_loss: 7.5091\n",
      "Epoch 201/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0797 - val_loss: 5.1129\n",
      "Epoch 202/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2209 - val_loss: 6.2091\n",
      "Epoch 203/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0973 - val_loss: 6.7322\n",
      "Epoch 204/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0800 - val_loss: 5.1657\n",
      "Epoch 205/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0643 - val_loss: 6.0445\n",
      "Epoch 206/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0252 - val_loss: 5.2966\n",
      "Epoch 207/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.9548 - val_loss: 5.1025\n",
      "Epoch 208/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9293 - val_loss: 5.3436\n",
      "Epoch 209/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9162 - val_loss: 5.6809\n",
      "Epoch 210/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9150 - val_loss: 5.4344\n",
      "Epoch 211/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9091 - val_loss: 5.2705\n",
      "Epoch 212/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9265 - val_loss: 5.2661\n",
      "Epoch 213/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9643 - val_loss: 5.3844\n",
      "Epoch 214/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3063 - val_loss: 7.0035\n",
      "Epoch 215/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9988 - val_loss: 4.7713\n",
      "Epoch 216/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1767 - val_loss: 5.5330\n",
      "Epoch 217/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9831 - val_loss: 6.2142\n",
      "Epoch 218/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.0970 - val_loss: 5.3403\n",
      "Epoch 219/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.9942 - val_loss: 5.4401\n",
      "Epoch 220/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9203 - val_loss: 5.7578\n",
      "Epoch 221/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9477 - val_loss: 5.3597\n",
      "Epoch 222/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9220 - val_loss: 5.3350\n",
      "Epoch 223/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9361 - val_loss: 5.5363\n",
      "Epoch 224/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9049 - val_loss: 5.4568\n",
      "Epoch 225/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8716 - val_loss: 5.4719\n",
      "Epoch 226/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8820 - val_loss: 5.0998\n",
      "Epoch 227/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8903 - val_loss: 5.0735\n",
      "Epoch 228/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8460 - val_loss: 5.7327\n",
      "Epoch 229/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8982 - val_loss: 5.0993\n",
      "Epoch 230/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8990 - val_loss: 5.2443\n",
      "Epoch 231/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9928 - val_loss: 5.7775\n",
      "Epoch 232/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9274 - val_loss: 5.1602\n",
      "Epoch 233/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9064 - val_loss: 5.5609\n",
      "Epoch 234/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8842 - val_loss: 5.6362\n",
      "Epoch 235/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.8603 - val_loss: 5.3051\n",
      "Epoch 236/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9013 - val_loss: 5.8198\n",
      "Epoch 237/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4396 - val_loss: 5.1932\n",
      "Epoch 238/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9665 - val_loss: 5.2758\n",
      "Epoch 239/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9376 - val_loss: 6.9039\n",
      "Epoch 240/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0612 - val_loss: 5.6875\n",
      "Epoch 241/250\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.9087 - val_loss: 5.2887\n",
      "Epoch 242/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.9166 - val_loss: 5.4671\n",
      "Epoch 243/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9627 - val_loss: 6.2905\n",
      "Epoch 244/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9192 - val_loss: 4.8331\n",
      "Epoch 245/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9097 - val_loss: 6.0282\n",
      "Epoch 246/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9726 - val_loss: 5.8152\n",
      "Epoch 247/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8605 - val_loss: 5.4948\n",
      "Epoch 248/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0171 - val_loss: 5.4133\n",
      "Epoch 249/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9027 - val_loss: 5.3050\n",
      "Epoch 250/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.9157 - val_loss: 5.1162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2272672e560>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, y_data = shuffle(X_train_array, y_train_array)\n",
    "\n",
    "\n",
    "input_dim = 12 # Please replace with your input dimension\n",
    "output_dim = 1 # Please replace with your output dimension\n",
    "\n",
    "# Define Multiply Network\n",
    "input_layer_1 = Input(shape=(input_dim,))\n",
    "input_layer_2 = Input(shape=(input_dim,))\n",
    "\n",
    "dense_1 = Dense(12000, activation='selu')(input_layer_1)\n",
    "dense_2 = Dense(12000, activation='selu')(input_layer_2)\n",
    "\n",
    "multiply_layer_1 = Multiply()([dense_1, dense_2])\n",
    "\n",
    "\n",
    "\n",
    "output_layer = Dense(output_dim, activation='linear')(multiply_layer_1)\n",
    "\n",
    "multiply_model = Model(inputs=[input_layer_1, input_layer_2], outputs=output_layer)\n",
    "\n",
    "multiply_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "multiply_model.fit([X_train_array, X_train_array], y_train_array, epochs=250, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/35 [..............................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#onehot_df_test_encoded, H_test_encoded = prepare_test_data(onehot_df_test, H_test)  # You need to ensure that your test data is prepared in the same way as your training data\n",
    "\n",
    "# Now you can make predictions on your test set\n",
    "predictions = multiply_model.predict([X_test_array,X_test_array])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):  22.8604938673812\n",
      "Root Mean Squared Error (RMSE):  4.781264881533045\n",
      "Mean Absolute Error (MAE):  3.216296166721244\n",
      "R-squared Score (R^2):  -0.1083833386763291\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE): ', mse)\n",
    "print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "print('Mean Absolute Error (MAE): ', mae)\n",
    "print('R-squared Score (R^2): ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot predicted vs real values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Real')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "differences = predictions.flatten() - y_test.flatten() # This will give the difference between your predictions and the actual values\n",
    "\n",
    "#Create a figure with two subplots: a histogram of the differences and a scatter plot of predicted vs real values\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(10, 15))\n",
    "\n",
    "#Plot histogram\n",
    "axs[0].hist(differences, bins=20, density=True)\n",
    "axs[0].set_title('Histogram of differences between predicted and actual values')\n",
    "axs[0].set_xlabel('Differences')\n",
    "axs[0].set_ylabel('Density')\n",
    "\n",
    "#Plot scatter\n",
    "axs[1].scatter(y_test, predictions, alpha=0.5)\n",
    "axs[1].set_title('Scatter plot of predicted vs actual values')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "\n",
    "#Draw a diagonal line on the scatterplot\n",
    "lims = [np.min([axs[1].get_xlim(), axs[1].get_ylim()]), # min of both axes\n",
    "np.max([axs[1].get_xlim(), axs[1].get_ylim()])] # max of both axes\n",
    "axs[1].plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "axs[1].set_xlim(lims)\n",
    "axs[1].set_ylim(lims)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multiply Simple Network\n",
    "input_layer_1_simple = Input(shape=(input_dim,))\n",
    "input_layer_2_simple = Input(shape=(input_dim,))\n",
    "\n",
    "multiply_layer_simple = Multiply()([input_layer_1_simple, input_layer_2_simple])\n",
    "\n",
    "output_layer_simple = Dense(output_dim, activation='linear')(multiply_layer_simple)\n",
    "\n",
    "multiply_simple_model = Model(inputs=[input_layer_1_simple, input_layer_2_simple], outputs=output_layer_simple)\n",
    "\n",
    "multiply_simple_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "multiply_simple_model.fit([train_data, train_data], y_train, epochs=150, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot_df_test_encoded, H_test_encoded = prepare_test_data(onehot_df_test, H_test)  # You need to ensure that your test data is prepared in the same way as your training data\n",
    "test_data = np.concatenate((onehot_df_test_encoded, H_test_encoded), axis = 1)\n",
    "# Now you can make predictions on your test set\n",
    "predictions = multiply_simple_model.predict([test_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE): ', mse)\n",
    "print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "print('Mean Absolute Error (MAE): ', mae)\n",
    "print('R-squared Score (R^2): ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs real values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Real')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "differences = predictions.flatten() - y_test.flatten() # This will give the difference between your predictions and the actual values\n",
    "\n",
    "#Create a figure with two subplots: a histogram of the differences and a scatter plot of predicted vs real values\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(10, 15))\n",
    "\n",
    "#Plot histogram\n",
    "axs[0].hist(differences, bins=20, density=True)\n",
    "axs[0].set_title('Histogram of differences between predicted and actual values')\n",
    "axs[0].set_xlabel('Differences')\n",
    "axs[0].set_ylabel('Density')\n",
    "\n",
    "#Plot scatter\n",
    "axs[1].scatter(y_test, predictions, alpha=0.5)\n",
    "axs[1].set_title('Scatter plot of predicted vs actual values')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "\n",
    "#Draw a diagonal line on the scatterplot\n",
    "lims = [np.min([axs[1].get_xlim(), axs[1].get_ylim()]), # min of both axes\n",
    "np.max([axs[1].get_xlim(), axs[1].get_ylim()])] # max of both axes\n",
    "axs[1].plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "axs[1].set_xlim(lims)\n",
    "axs[1].set_ylim(lims)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
