{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:30:08.002485800Z",
     "start_time": "2023-06-18T21:30:06.192906300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def identity(x):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def logit(x):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/sandra/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "\u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "2023-06-18 23:30:06.851781: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-18 23:30:06.853376: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-18 23:30:06.885750: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-18 23:30:06.886952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 23:30:07.476802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fboost import outlier_iqr, DataPreparator, FeatureBoosterRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from ffx import FFXRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value\n",
    "seed_value= 0\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:30:08.020070Z",
     "start_time": "2023-06-18T21:30:08.002485800Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw/alldata.csv')\n",
    "data.columns = ['familia','indice_fam','subposicion_1','subposicion_2','subposicion_3','subposicion_4', 'energia']\n",
    "mask = (data[['subposicion_1','subposicion_2','subposicion_3','subposicion_4']] == 0).sum(axis=1) > 1\n",
    "lista_train = data.loc[mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]\n",
    "lista_test = data.loc[~mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:30:08.051326700Z",
     "start_time": "2023-06-18T21:30:08.020070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "X_train = lista_train.drop(['energia'], axis=1)\n",
    "X_test = lista_test.drop(['energia'], axis=1)\n",
    "\n",
    "# Fit the encoder and transform the data for both train and test dataframes\n",
    "encoded_train = encoder.fit_transform(X_train)\n",
    "encoded_test = encoder.transform(X_test)\n",
    "\n",
    "# Now, 'encoded_train' and 'encoded_test' are numpy arrays, we can convert them back to dataframes:\n",
    "X_train_scaled = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(X_train.columns))\n",
    "X_test_scaled = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(X_test.columns))\n",
    "\n",
    "\n",
    "y_test = lista_test['energia']\n",
    "y_train = lista_train['energia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T21:30:08.053321100Z",
     "start_time": "2023-06-18T21:30:08.031051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     familia indice_fam subposicion_1 subposicion_2 subposicion_3  \\\n",
      "0        one        one          zero          zero          zero   \n",
      "1        one        two          zero          zero           one   \n",
      "2        one      three           one          zero          zero   \n",
      "3        one       four          zero           one          zero   \n",
      "4        two        one          zero          zero          zero   \n",
      "...      ...        ...           ...           ...           ...   \n",
      "1283     342        two          five           two          five   \n",
      "1284     343        one         three          four         three   \n",
      "1285     343        two          four         three          four   \n",
      "1286     344        one         three          five         three   \n",
      "1287     344        two          five         three          five   \n",
      "\n",
      "     subposicion_4    energia  \n",
      "0              one  1.2901211  \n",
      "1             zero  1.2901211  \n",
      "2             zero  1.2901211  \n",
      "3             zero  1.2901211  \n",
      "4              two  0.0606045  \n",
      "...            ...        ...  \n",
      "1283           two   9.102429  \n",
      "1284          four  5.9629837  \n",
      "1285         three  5.9629837  \n",
      "1286          five  6.1082563  \n",
      "1287         three  6.1082563  \n",
      "\n",
      "[1288 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have a DataFrame `df` with 'sentence' and 'score' columns\n",
    "df = pd.read_csv('../data/raw/alldata.csv')\n",
    "# Converting all columns to string\n",
    "df = df.astype(str)\n",
    "\n",
    "df2 = df.replace('0','zero')\n",
    "df2 = df2.replace('1','one')\n",
    "df2 = df2.replace('2','two')\n",
    "df2 = df2.replace('3','three')\n",
    "df2 = df2.replace('4','four')\n",
    "df2 = df2.replace('5','five')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Convert DataFrame where each column is a word to a series of strings\n",
    "df2['sentence'] = df2.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(df2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Specify your dictionary of words (customize n here)\n",
    "dictionary = ['zero', 'one', 'two', 'three', 'four', 'five']\n",
    "\n",
    "# Initialize TfidfVectorizer with your dictionary\n",
    "# TfidfVectorizer will convert sentences into vectors of length n\n",
    "vectorizer = TfidfVectorizer(vocabulary=dictionary)\n",
    "\n",
    "# Transform the sentences in train and test data into vectors\n",
    "X_train = vectorizer.fit_transform(train_data['sentence']).toarray()\n",
    "X_test = vectorizer.transform(test_data['sentence']).toarray()\n",
    "\n",
    "# Get the scores\n",
    "y_train = train_data['energia']\n",
    "y_test = test_data['energia']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:30:08.108257600Z",
     "start_time": "2023-06-18T21:30:08.052321700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create the individual models\n",
    "model1 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model2 = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model3 = make_pipeline(PolynomialFeatures(22), LinearRegression())\n",
    "model4 = LinearRegression()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble = VotingRegressor([('rf', model1), ('gb', model2), ('poly', model3), ('lr', model4)])\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Now you can make predictions with the ensemble model\n",
    "predictions = ensemble.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.277057300Z",
     "start_time": "2023-06-18T21:30:08.108257600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (MSE):  -3.5052415016803422e+22\n",
      "Mean Squared Error (MSE):  7.579465354034085e+23\n",
      "Root Mean Squared Error (RMSE):  870601249369.3129\n",
      "Mean Absolute Error (MAE):  135513689701.41672\n",
      "R-squared Score (R^2):  -3.5052415016803422e+22\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and score the model\n",
    "score = ensemble.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print('Score (MSE): ', score)\n",
    "print('Mean Squared Error (MSE): ', mse)\n",
    "print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "print('Mean Absolute Error (MAE): ', mae)\n",
    "print('R-squared Score (R^2): ', r2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.570964300Z",
     "start_time": "2023-06-18T21:31:58.277057300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     subposicion_1 subposicion_2 subposicion_3 subposicion_4  \\\n",
      "0             zero          zero          zero           one   \n",
      "1             zero          zero           one          zero   \n",
      "2              one          zero          zero          zero   \n",
      "3             zero           one          zero          zero   \n",
      "4             zero          zero          zero           two   \n",
      "...            ...           ...           ...           ...   \n",
      "1265         three          zero         three          zero   \n",
      "1266          zero          four          zero          four   \n",
      "1267          four          zero          four          zero   \n",
      "1268          zero          five          zero          five   \n",
      "1269          five          zero          five          zero   \n",
      "\n",
      "                   sentence  \n",
      "0        zero zero zero one  \n",
      "1        zero zero one zero  \n",
      "2        one zero zero zero  \n",
      "3        zero one zero zero  \n",
      "4        zero zero zero two  \n",
      "...                     ...  \n",
      "1265  three zero three zero  \n",
      "1266    zero four zero four  \n",
      "1267    four zero four zero  \n",
      "1268    zero five zero five  \n",
      "1269    five zero five zero  \n",
      "\n",
      "[170 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have a DataFrame `df` with 'sentence' and 'score' columns\n",
    "df = pd.read_csv('../data/raw/alldata.csv')\n",
    "\n",
    "mask = (data[['subposicion_1','subposicion_2','subposicion_3','subposicion_4']] == 0).sum(axis=1) > 1\n",
    "lista_train = data.loc[mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]\n",
    "lista_test = data.loc[~mask,['subposicion_1','subposicion_2','subposicion_3','subposicion_4','energia']]\n",
    "\n",
    "X_train = lista_train.drop(['energia'], axis=1)\n",
    "X_test = lista_test.drop(['energia'], axis=1)\n",
    "\n",
    "y_test = lista_test['energia']\n",
    "y_train = lista_train['energia']\n",
    "# Converting all columns to string\n",
    "X_train = X_train.astype(str)\n",
    "\n",
    "X_train = X_train.replace('0','zero')\n",
    "X_train = X_train.replace('1','one')\n",
    "X_train = X_train.replace('2','two')\n",
    "X_train = X_train.replace('3','three')\n",
    "X_train = X_train.replace('4','four')\n",
    "X_train = X_train.replace('5','five')\n",
    "\n",
    "# Convert DataFrame where each column is a word to a series of strings, excluding 'energia' column\n",
    "X_train['sentence'] = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "print(X_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.653206500Z",
     "start_time": "2023-06-18T21:31:58.570456800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "NUM_WORDS = 1000\n",
    "EMBEDDING_DIM = 2\n",
    "MAXLEN = 4\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "TRAINING_SPLIT = .8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.654206100Z",
     "start_time": "2023-06-18T21:31:58.646174500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "\n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    words = sentence.split()\n",
    "    no_words = [w for w in words if w not in stopwords]\n",
    "    sentence = \" \".join(no_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def parse_data_from_dataframe(input_data):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "\n",
    "    Args:\n",
    "        input_data (string): name of the dataframe\n",
    "\n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    sentence_list = []\n",
    "    label_list = []\n",
    "    for index, row in input_data.iterrows():\n",
    "        #label_list.append(row['energia'])\n",
    "        sentence_ind = row['sentence']\n",
    "        sentence_ind = remove_stopwords(sentence_ind)\n",
    "        sentence_list.append(sentence_ind)\n",
    "\n",
    "    #return sentence_list, label_list\n",
    "    return sentence_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.654206100Z",
     "start_time": "2023-06-18T21:31:58.646174500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero zero zero one', 'zero zero one zero', 'one zero zero zero', 'zero one zero zero', 'zero zero zero two', 'zero zero two zero', 'two zero zero zero', 'zero two zero zero', 'zero zero zero three', 'zero zero three zero', 'three zero zero zero', 'zero three zero zero', 'zero zero zero four', 'zero zero four zero', 'four zero zero zero', 'zero four zero zero', 'zero zero zero five', 'zero zero five zero', 'five zero zero zero', 'zero five zero zero', 'zero zero one two', 'zero zero two one', 'two one zero zero', 'one two zero zero', 'zero zero one three', 'zero zero three one', 'three one zero zero', 'one three zero zero', 'zero zero one four', 'zero zero four one', 'four one zero zero', 'one four zero zero', 'zero zero one five', 'zero zero five one', 'five one zero zero', 'one five zero zero', 'zero zero two three', 'zero zero three two', 'three two zero zero', 'two three zero zero', 'zero zero two four', 'zero zero four two', 'four two zero zero', 'two four zero zero', 'zero zero two five', 'zero zero five two', 'five two zero zero', 'two five zero zero', 'zero zero three four', 'zero zero four three', 'four three zero zero', 'three four zero zero', 'zero zero three five', 'zero zero five three', 'five three zero zero', 'three five zero zero', 'zero zero four five', 'zero zero five four', 'five four zero zero', 'four five zero zero', 'zero one zero two', 'one zero two zero', 'two zero one zero', 'zero two zero one', 'zero one zero three', 'one zero three zero', 'three zero one zero', 'zero three zero one', 'zero one zero four', 'one zero four zero', 'four zero one zero', 'zero four zero one', 'zero one zero five', 'one zero five zero', 'five zero one zero', 'zero five zero one', 'zero two zero three', 'two zero three zero', 'three zero two zero', 'zero three zero two', 'zero two zero four', 'two zero four zero', 'four zero two zero', 'zero four zero two', 'zero two zero five', 'two zero five zero', 'five zero two zero', 'zero five zero two', 'zero three zero four', 'three zero four zero', 'four zero three zero', 'zero four zero three', 'zero three zero five', 'three zero five zero', 'five zero three zero', 'zero five zero three', 'zero four zero five', 'four zero five zero', 'five zero four zero', 'zero five zero four', 'one zero zero two', 'zero one two zero', 'two zero zero one', 'zero two one zero', 'one zero zero three', 'zero one three zero', 'three zero zero one', 'zero three one zero', 'one zero zero four', 'zero one four zero', 'four zero zero one', 'zero four one zero', 'one zero zero five', 'zero one five zero', 'five zero zero one', 'zero five one zero', 'two zero zero three', 'zero two three zero', 'three zero zero two', 'zero three two zero', 'two zero zero four', 'zero two four zero', 'four zero zero two', 'zero four two zero', 'two zero zero five', 'zero two five zero', 'five zero zero two', 'zero five two zero', 'three zero zero four', 'zero three four zero', 'four zero zero three', 'zero four three zero', 'three zero zero five', 'zero three five zero', 'five zero zero three', 'zero five three zero', 'four zero zero five', 'zero four five zero', 'five zero zero four', 'zero five four zero', 'zero zero one one', 'one one zero zero', 'zero zero two two', 'two two zero zero', 'zero zero three three', 'three three zero zero', 'zero zero four four', 'four four zero zero', 'zero zero five five', 'five five zero zero', 'one zero zero one', 'zero one one zero', 'two zero zero two', 'zero two two zero', 'three zero zero three', 'zero three three zero', 'four zero zero four', 'zero four four zero', 'five zero zero five', 'zero five five zero', 'zero one zero one', 'one zero one zero', 'zero two zero two', 'two zero two zero', 'zero three zero three', 'three zero three zero', 'zero four zero four', 'four zero four zero', 'zero five zero five', 'five zero five zero']\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "#sentences, labels_array = parse_data_from_dataframe(X_train)\n",
    "sentences = parse_data_from_dataframe(X_train)\n",
    "print(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.673277100Z",
     "start_time": "2023-06-18T21:31:58.653206500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 170 sentences in the dataset.\n",
      "\n",
      "First sentence has 4 words (after removing stopwords).\n",
      "\n",
      "There are 170 labels in the dataset.\n",
      "\n",
      "The first 5 labels are [1.2901211 1.2901211 1.2901211 1.2901211 0.0606045]\n"
     ]
    }
   ],
   "source": [
    "#labels = [float(x) for x in labels_array]\n",
    "labels = np.array(y_train)\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.722930300Z",
     "start_time": "2023-06-18T21:31:58.673277100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# GRADED FUNCTIONS: train_val_split\n",
    "def train_val_split(sentences, labels, training_split):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets\n",
    "\n",
    "    Args:\n",
    "        sentences (list of string): lower-cased sentences without stopwords\n",
    "        labels (list of string): list of labels\n",
    "        training split (float): proportion of the dataset to convert to include in the train set\n",
    "\n",
    "    Returns:\n",
    "        train_sentences, validation_sentences, train_labels, validation_labels - lists containing the data splits\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Compute the number of sentences that will be used for training (should be an integer)\n",
    "    train_size = int(len(sentences)*training_split)\n",
    "\n",
    "    # Split the sentences and labels into train/validation splits\n",
    "    train_sentences = sentences[0:train_size]\n",
    "    train_labels = labels[0:train_size]\n",
    "\n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.744971400Z",
     "start_time": "2023-06-18T21:31:58.682239700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 136 sentences for training.\n",
      "\n",
      "There are 136 labels for training.\n",
      "\n",
      "There are 34 sentences for validation.\n",
      "\n",
      "There are 34 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_val_split(X_train, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.745979500Z",
     "start_time": "2023-06-18T21:31:58.690354100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
    "    \"\"\"\n",
    "    Instantiates the Tokenizer class on the training sentences\n",
    "\n",
    "    Args:\n",
    "        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n",
    "        num_words (int) - number of words to keep when tokenizing\n",
    "        oov_token (string) - symbol for the out-of-vocabulary token\n",
    "\n",
    "    Returns:\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token\n",
    "    tokenizer = Tokenizer(num_words = num_words, oov_token=oov_token)\n",
    "\n",
    "    # Fit the tokenizer to the training sentences\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.745979500Z",
     "start_time": "2023-06-18T21:31:58.700381300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 7 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.745979500Z",
     "start_time": "2023-06-18T21:31:58.711409600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
    "    \"\"\"\n",
    "    Generates an array of token sequences and pads them to the same length\n",
    "\n",
    "    Args:\n",
    "        sentences (list of string): list of sentences to tokenize and pad\n",
    "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
    "        padding (string): type of padding to use\n",
    "        maxlen (int): maximum length of the token sequence\n",
    "\n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Convert sentences to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    # Pad the sequences using the correct padding and maxlen\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding)\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return padded_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.746971100Z",
     "start_time": "2023-06-18T21:31:58.722930300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (5, 4)\n",
      "\n",
      "Padded validation sequences have shape: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.746971100Z",
     "start_time": "2023-06-18T21:31:58.723924700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: tokenize_labels\n",
    "def tokenize_labels(all_labels, split_labels):\n",
    "    \"\"\"\n",
    "    Tokenizes the labels\n",
    "\n",
    "    Args:\n",
    "        all_labels (list of string): labels to generate the word-index from\n",
    "        split_labels (list of string): labels to tokenize\n",
    "\n",
    "    Returns:\n",
    "        label_seq_np (array of int): tokenized labels\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Instantiate the Tokenizer (no additional arguments needed)\n",
    "    label_tokenizer = Tokenizer()\n",
    "\n",
    "    # Fit the tokenizer on all the labels\n",
    "    label_tokenizer.fit_on_texts(all_labels)\n",
    "\n",
    "    # Convert labels to sequences\n",
    "    label_seq = label_tokenizer.texts_to_sequences(split_labels)\n",
    "\n",
    "    # Convert sequences to a numpy array and subtract 1 from each entry\n",
    "    label_seq_np = np.array(label_seq) - 1\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return label_seq_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.746971100Z",
     "start_time": "2023-06-18T21:31:58.732453500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels of the training set should look like this:\n",
      "[1.2901211 1.2901211 1.2901211 1.2901211 0.0606045]\n",
      "\n",
      "First 5 labels of the validation set should look like this:\n",
      "[0.8130417 0.8130417 0.8130417 0.8130417 4.4875204]\n",
      "\n",
      "Tokenized labels of the training set have shape: (136,)\n",
      "\n",
      "Tokenized labels of the validation set have shape: (34,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_label_seq = np.array(train_labels)\n",
    "val_label_seq = np.array(val_labels)\n",
    "\n",
    "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
    "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
    "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
    "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.765019800Z",
     "start_time": "2023-06-18T21:31:58.740060200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \"\"\"\n",
    "    Creates a text classifier model\n",
    "\n",
    "    Args:\n",
    "        num_words (int): size of the vocabulary for the Embedding layer input\n",
    "        embedding_dim (int): dimensionality of the Embedding layer output\n",
    "        maxlen (int): length of the input sequences\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras Model): the text classifier model\n",
    "    \"\"\"\n",
    "\n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:58.833106600Z",
     "start_time": "2023-06-18T21:31:58.750502200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 5\n  y sizes: 136\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n\u001B[0;32m----> 3\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_padded_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_label_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mval_padded_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_label_seq\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/deedevelopment.ai/resmol/venv/lib/python3.10/site-packages/keras/engine/data_adapter.py:1852\u001B[0m, in \u001B[0;36m_check_data_cardinality\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m   1845\u001B[0m     msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m sizes: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1846\u001B[0m         label,\n\u001B[1;32m   1847\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m   1848\u001B[0m             \u001B[38;5;28mstr\u001B[39m(i\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(single_data)\n\u001B[1;32m   1849\u001B[0m         ),\n\u001B[1;32m   1850\u001B[0m     )\n\u001B[1;32m   1851\u001B[0m msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMake sure all arrays contain the same number of samples.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1852\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[0;31mValueError\u001B[0m: Data cardinality is ambiguous:\n  x sizes: 5\n  y sizes: 136\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n",
    "\n",
    "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T21:31:59.613763100Z",
     "start_time": "2023-06-18T21:31:58.758502700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE): ', mse)\n",
    "print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "print('Mean Absolute Error (MAE): ', mae)\n",
    "print('R-squared Score (R^2): ', r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume you have a DataFrame `df` with 'sentence' and 'score' columns\n",
    "df = pd.read_csv('../data/raw/alldata.csv')\n",
    "# Converting all columns to string\n",
    "df = df.astype(str)\n",
    "\n",
    "df2 = df.replace('0','zero')\n",
    "df2 = df2.replace('1','one')\n",
    "df2 = df2.replace('2','two')\n",
    "df2 = df2.replace('3','three')\n",
    "df2 = df2.replace('4','four')\n",
    "df2 = df2.replace('5','five')\n",
    "\n",
    "# Convert DataFrame where each column is a word to a series of strings, excluding 'energia' column\n",
    "df2['sentence'] = df2.drop(['familia','indice_fam','energia'], axis=1).apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(df2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Parameters\n",
    "max_length = 100  # Maximum length of sentences\n",
    "embedding_dim = 100  # Dimensionality of word embeddings\n",
    "vocab_size = 10000  # Maximum number of words in the vocabulary\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_data['sentence'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['sentence'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['sentence'])\n",
    "\n",
    "# Pad the sequences to have a consistent length\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Load pre-trained word embeddings (e.g., GloVe)\n",
    "# Replace 'path_to_embeddings' with the actual path to your pre-trained embeddings file\n",
    "embeddings_index = {}\n",
    "with open('path_to_embeddings/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(max_length, embedding_dim))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, train_data['score'], epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(test_data['score'], predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
